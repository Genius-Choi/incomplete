def _export_annotations(
    db_instance: models.Project | models.Task | models.Job,
    rq_id: str,
    request: HttpRequest,
    format_name: str,
    action: str,
    callback: Callable[[int, Optional[str], Optional[str]], str],
    filename: Optional[str],
    location_conf: Dict[str, Any]
):
    if action not in {"", "download"}:
        raise serializers.ValidationError(
            "Unexpected action specified for the request")

    format_desc = {f.DISPLAY_NAME: f
        for f in dm.views.get_export_formats()}.get(format_name)
    if format_desc is None:
        raise serializers.ValidationError(
            "Unknown format specified for the request")
    elif not format_desc.ENABLED:
        return Response(status=status.HTTP_405_METHOD_NOT_ALLOWED)

    instance_update_time = timezone.localtime(db_instance.updated_date)
    if isinstance(db_instance, Project):
        tasks_update = list(map(lambda db_task: timezone.localtime(db_task.updated_date), db_instance.tasks.all()))
        instance_update_time = max(tasks_update + [instance_update_time])

    queue = django_rq.get_queue(settings.CVAT_QUEUES.EXPORT_DATA.value)
    rq_job = queue.fetch_job(rq_id)

    if rq_job:
        rq_request = rq_job.meta.get('request', None)
        request_time = rq_request.get('timestamp', None) if rq_request else None
        if request_time is None or request_time < instance_update_time:
            # The result is outdated, need to restart the export.
            # Cancel the current job.
            # The new attempt will be made after the last existing job.
            # In the case the server is configured with ONE_RUNNING_JOB_IN_QUEUE_PER_USER
            # we have to enqueue dependent jobs after canceling one.
            rq_job.cancel(enqueue_dependents=settings.ONE_RUNNING_JOB_IN_QUEUE_PER_USER)
            rq_job.delete()
            rq_job = None

    location = location_conf.get('location')
    if location not in Location.list():
        raise serializers.ValidationError(
            f"Unexpected location {location} specified for the request"
        )

    cache_ttl = dm.views.get_export_cache_ttl(db_instance)

    instance_timestamp = datetime.strftime(instance_update_time, "%Y_%m_%d_%H_%M_%S")
    is_annotation_file = rq_id.startswith('export:annotations')

    REQUEST_TIMEOUT = 60

    if action == "download":
        if location != Location.LOCAL:
            return Response('Action "download" is only supported for a local export location',
                status=status.HTTP_400_BAD_REQUEST)

        if not rq_job or not rq_job.is_finished:
            return Response('Export has not finished', status=status.HTTP_400_BAD_REQUEST)

        file_path = rq_job.return_value()

        if not file_path:
            return Response(
                'A result for exporting job was not found for finished RQ job',
                status=status.HTTP_500_INTERNAL_SERVER_ERROR
            )

        with dm.util.get_export_cache_lock(file_path, ttl=REQUEST_TIMEOUT):
            if not osp.exists(file_path):
                return Response(
                    "The exported file has expired, please retry exporting",
                    status=status.HTTP_404_NOT_FOUND
                )

            filename = filename or \
                build_annotations_file_name(
                    class_name=db_instance.__class__.__name__,
                    identifier=db_instance.name if isinstance(db_instance, (Task, Project)) else db_instance.id,
                    timestamp=instance_timestamp,
                    format_name=format_name,
                    is_annotation_file=is_annotation_file,
                    extension=osp.splitext(file_path)[1]
                )

            rq_job.delete()
            return sendfile(request, file_path, attachment=True, attachment_filename=filename)


    if rq_job:
        if rq_job.is_finished:
            if location == Location.CLOUD_STORAGE:
                rq_job.delete()
                return Response(status=status.HTTP_200_OK)

            elif location == Location.LOCAL:
                file_path = rq_job.return_value()

                if not file_path:
                    return Response(
                        'A result for exporting job was not found for finished RQ job',
                        status=status.HTTP_500_INTERNAL_SERVER_ERROR
                    )

                with dm.util.get_export_cache_lock(file_path, ttl=REQUEST_TIMEOUT):
                    if osp.exists(file_path):
                        # Update last update time to prolong the export lifetime
                        # as the last access time is not available on every filesystem
                        os.utime(file_path, None)

                        return Response(status=status.HTTP_201_CREATED)
                    else:
                        # Cancel and reenqueue the job.
                        # The new attempt will be made after the last existing job.
                        # In the case the server is configured with ONE_RUNNING_JOB_IN_QUEUE_PER_USER
                        # we have to enqueue dependent jobs after canceling one.
                        rq_job.cancel(enqueue_dependents=settings.ONE_RUNNING_JOB_IN_QUEUE_PER_USER)
                        rq_job.delete()
            else:
                raise NotImplementedError(f"Export to {location} location is not implemented yet")
        elif rq_job.is_failed:
            exc_info = rq_job.meta.get('formatted_exception', str(rq_job.exc_info))
            rq_job.delete()
            return Response(exc_info, status=status.HTTP_500_INTERNAL_SERVER_ERROR)
        elif rq_job.is_deferred and rq_id not in queue.deferred_job_registry.get_job_ids():
            # Sometimes jobs can depend on outdated jobs in the deferred jobs registry.
            # They can be fetched by their specific ids, but are not listed by get_job_ids().
            # Supposedly, this can happen because of the server restarts
            # (potentially, because the redis used for the queue is inmemory).
            # Another potential reason is canceling without enqueueing dependents.
            # Such dependencies are never removed or finished,
            # as there is no TTL for deferred jobs,
            # so the current job can be blocked indefinitely.

            # Cancel the current job and then reenqueue it, considering the current situation.
            # The new attempt will be made after the last existing job.
            # In the case the server is configured with ONE_RUNNING_JOB_IN_QUEUE_PER_USER
            # we have to enqueue dependent jobs after canceling one.
            rq_job.cancel(enqueue_dependents=settings.ONE_RUNNING_JOB_IN_QUEUE_PER_USER)
            rq_job.delete()
        else:
            return Response(status=status.HTTP_202_ACCEPTED)
    try:
        if request.scheme:
            server_address = request.scheme + '://'
        server_address += request.get_host()
    except Exception:
        server_address = None

    user_id = request.user.id

    func = callback if location == Location.LOCAL else export_resource_to_cloud_storage
    func_args = (db_instance.id, format_name, server_address)

    if location == Location.CLOUD_STORAGE:
        try:
            storage_id = location_conf['storage_id']
        except KeyError:
            raise serializers.ValidationError(
                'Cloud storage location was selected as the destination,'
                ' but cloud storage id was not specified')

        db_storage = get_cloud_storage_for_import_or_export(
            storage_id=storage_id, request=request,
            is_default=location_conf['is_default'])
        filename_pattern = build_annotations_file_name(
            class_name=db_instance.__class__.__name__,
            identifier=db_instance.name if isinstance(db_instance, (Task, Project)) else db_instance.id,
            timestamp=instance_timestamp,
            format_name=format_name,
            is_annotation_file=is_annotation_file,
        )
        func_args = (db_storage, filename, filename_pattern, callback) + func_args
    else:
        db_storage = None

    with get_rq_lock_by_user(queue, user_id):
        queue.enqueue_call(
            func=func,
            args=func_args,
            job_id=rq_id,
            meta=get_rq_job_meta(request=request, db_obj=db_instance),
            depends_on=define_dependent_job(queue, user_id, rq_id=rq_id),
            result_ttl=cache_ttl.total_seconds(),
            failure_ttl=cache_ttl.total_seconds(),
        )

    handle_dataset_export(db_instance,
        format_name=format_name, cloud_storage=db_storage, save_images=not is_annotation_file)

    return Response(status=status.HTTP_202_ACCEPTED)
