tsl::Status CudnnSupport::GetFusedMatmulRunners(
    bool use_cudnn_frontend, dnn::DataType input_type, dnn::DataType bias_type,
    dnn::DataType output_type, Stream* stream, bool trans_a, bool trans_b,
    uint64_t m, uint64_t n, uint64_t k, int64_t lda, int64_t ldb, int64_t ldc,
    dnn::ActivationMode activation_mode, bool use_fallback,
    std::vector<std::unique_ptr<const dnn::FusedMatmulRunner>>*
        out_exec_plans) {
#if CUDNN_VERSION >= 8400 && TF_ENABLE_CUDNN_FRONTEND
  if (!use_cudnn_frontend) {
    return tsl::errors::Unimplemented(
        "Cudnn execution plans for matmul are only supported with cudnn "
        "frontend APIs.");
  }

  auto cudnn = cudnn_->GetHandle(parent_, stream);
  auto op_graph_status = GetCudnnFusedMatmulGraph(
      input_type, bias_type, output_type, trans_a, trans_b, m, n, k, lda, ldb,
      ldc, activation_mode, cudnn);
  if (!op_graph_status.status().ok()) {
    return tsl::Status(tsl::error::INTERNAL,
                       absl::StrCat("Cudnn graph failed to build: ",
                                    op_graph_status.status().ToString()));
  }
  auto op_graph = std::move(op_graph_status).value();

  // The "need_side_input" will not actually affect the matmul execution. It was
  // proposed to work around a convolution issue with five inputs (see
  // SideInputNeeded()). Here, we set it true to make sure none of the inputs
  // get dropped in case the number of inputs get increased in the future.
  return CreateOpRunners<dnn::FusedMatmulSignature>(
      stream, cudnn, parent_, cudnn_.get(), std::move(op_graph),
      dnn::ConvolutionKind::INVALID, input_type, {'a', 'b', 'z', 'c'},
      use_fallback, out_exec_plans, /*need_side_input=*/true);
#else
  return tsl::errors::Unimplemented(
      "Cudnn execution plans for matmul are only supported with Cudnn >= 8.4.");
#endif  // CUDNN_VERSION >= 8400 && TF_ENABLE_CUDNN_FRONTEND
}
