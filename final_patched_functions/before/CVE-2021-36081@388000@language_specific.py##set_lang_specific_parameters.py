def set_lang_specific_parameters(ctx, lang):
    # The default text location is now given directly from the language code.
    TEXT_CORPUS = f"{FLAGS_webtext_prefix}/{lang}.corpus.txt"
    FILTER_ARGUMENTS = []
    WORDLIST2DAWG_ARGUMENTS = ""
    # These dawg factors represent the fraction of the corpus not covered by the
    # dawg, and seem like reasonable defaults, but the optimal value is likely
    # to be highly corpus-dependent, as well as somewhat language-dependent.
    # Number dawg factor is the fraction of all numeric strings that are not
    # covered, which is why it is higher relative to the others.
    PUNC_DAWG_FACTOR = None
    NUMBER_DAWG_FACTOR = 0.125
    WORD_DAWG_FACTOR = 0.05
    BIGRAM_DAWG_FACTOR = 0.015
    TRAINING_DATA_ARGUMENTS = []
    FRAGMENTS_DISABLED = "y"
    RUN_SHAPE_CLUSTERING = False
    AMBIGS_FILTER_DENOMINATOR = "100000"
    LEADING = 32
    MEAN_COUNT = 40  # Default for latin script.
    # Language to mix with the language for maximum accuracy. Defaults to eng.
    # If no language is good, set to the base language.
    MIX_LANG = "eng"
    FONTS = ctx.fonts
    TEXT2IMAGE_EXTRA_ARGS = []
    EXPOSURES = []

    GENERATE_WORD_BIGRAMS = None
    WORD_DAWG_SIZE = None

    # Latin languages.
    if lang == "enm":
        TEXT2IMAGE_EXTRA_ARGS += ["--ligatures"]  # Add ligatures when supported
        if not FONTS:
            FONTS = EARLY_LATIN_FONTS
    elif lang == "frm":
        TEXT_CORPUS = f"{FLAGS_webtext_prefix}/fra.corpus.txt"
        # Make long-s substitutions for Middle French text
        FILTER_ARGUMENTS += ["--make_early_language_variant=fra"]
        TEXT2IMAGE_EXTRA_ARGS += ["--ligatures"]  # Add ligatures when supported.
        if not FONTS:
            FONTS = EARLY_LATIN_FONTS
    elif lang == "frk":
        TEXT_CORPUS = f"{FLAGS_webtext_prefix}/deu.corpus.txt"
        if not FONTS:
            FONTS = FRAKTUR_FONTS
    elif lang == "ita_old":
        TEXT_CORPUS = f"{FLAGS_webtext_prefix}/ita.corpus.txt"
        # Make long-s substitutions for Early Italian text
        FILTER_ARGUMENTS += ["--make_early_language_variant=ita"]
        TEXT2IMAGE_EXTRA_ARGS += ["--ligatures"]  # Add ligatures when supported.
        if not FONTS:
            FONTS = EARLY_LATIN_FONTS
    elif lang == "lat":
        if not EXPOSURES:
            EXPOSURES = "-3 -2 -1 0 1 2 3".split()
        if not FONTS:
            FONTS = NEOLATIN_FONTS
    elif lang == "spa_old":
        TEXT_CORPUS = f"{FLAGS_webtext_prefix}/spa.corpus.txt"
        # Make long-s substitutions for Early Spanish text
        FILTER_ARGUMENTS += ["--make_early_language_variant=spa"]
        TEXT2IMAGE_EXTRA_ARGS += ["--ligatures"]  # Add ligatures when supported.
        if not FONTS:
            FONTS = EARLY_LATIN_FONTS
    elif lang == "srp_latn":
        TEXT_CORPUS = f"{FLAGS_webtext_prefix}/srp.corpus.txt"
    elif lang == "vie":
        TRAINING_DATA_ARGUMENTS += ["--infrequent_ratio=10000"]
        if not FONTS:
            FONTS = VIETNAMESE_FONTS
        # Highly inflective languages get a bigger dawg size.
        # TODO(rays) Add more here!
    elif lang == "hun":
        WORD_DAWG_SIZE = 1_000_000
    elif lang == "pol":
        WORD_DAWG_SIZE = 1_000_000

        # Latin with default treatment.
    elif lang == "afr":
        pass
    elif lang == "aze":
        pass
    elif lang == "bos":
        pass
    elif lang == "cat":
        pass
    elif lang == "ceb":
        pass
    elif lang == "ces":
        PUNC_DAWG_FACTOR = 0.004
    elif lang == "cym":
        pass
    elif lang == "dan":
        pass
    elif lang == "deu":
        WORD_DAWG_FACTOR = 0.125
    elif lang == "eng":
        WORD_DAWG_FACTOR = 0.03
    elif lang == "epo":
        pass
    elif lang == "est":
        pass
    elif lang == "eus":
        pass
    elif lang == "fil":
        pass
    elif lang == "fin":
        pass
    elif lang == "fra":
        WORD_DAWG_FACTOR = 0.08
    elif lang == "gle":
        pass
    elif lang == "gle_uncial":
        if not FONTS:
            FONTS = IRISH_UNCIAL_FONTS
    elif lang == "glg":
        pass
    elif lang == "hat":
        pass
    elif lang == "hrv":
        pass
    elif lang == "iast":
        pass
    elif lang == "ind":
        pass
    elif lang == "isl":
        pass
    elif lang == "ita":
        pass
    elif lang == "jav":
        pass
    elif lang == "lav":
        pass
    elif lang == "lit":
        pass
    elif lang == "mlt":
        pass
    elif lang == "msa":
        pass
    elif lang == "nld":
        WORD_DAWG_FACTOR = 0.02
    elif lang == "nor":
        pass
    elif lang == "por":
        pass
    elif lang == "ron":
        pass
    elif lang == "slk":
        pass
    elif lang == "slv":
        pass
    elif lang == "spa":
        pass
    elif lang == "sqi":
        pass
    elif lang == "swa":
        pass
    elif lang == "swe":
        pass
    elif lang == "tgl":
        pass
    elif lang == "tur":
        pass
    elif lang == "uzb":
        pass
    elif lang == "zlm":
        pass

        # Special code for performing language-id that is trained on
        # EFIGS+Latin+Vietnamese text with regular + fraktur fonts.
    elif lang == "lat_lid":
        TEXT_CORPUS = f"{FLAGS_webtext_prefix}/lat_lid.corpus.txt"
        TRAINING_DATA_ARGUMENTS += ["--infrequent_ratio=10000"]
        GENERATE_WORD_BIGRAMS = 0
        # Strip unrenderable words as not all fonts will render the extended
        # latin symbols found in Vietnamese text.
        WORD_DAWG_SIZE = 1_000_000
        if not FONTS:
            FONTS = EARLY_LATIN_FONTS

        # Cyrillic script-based languages. It is bad to mix Latin with Cyrillic.
    elif lang == "rus":
        if not FONTS:
            FONTS = RUSSIAN_FONTS
        MIX_LANG = "rus"
        NUMBER_DAWG_FACTOR = 0.05
        WORD_DAWG_SIZE = 1_000_000
    elif lang in (
            "aze_cyrl",
            "bel",
            "bul",
            "kaz",
            "mkd",
            "srp",
            "tgk",
            "ukr",
            "uzb_cyrl",
    ):
        MIX_LANG = f"{lang}"
        if not FONTS:
            FONTS = RUSSIAN_FONTS

        # Special code for performing Cyrillic language-id that is trained on
        # Russian, Serbian, Ukrainian, Belarusian, Macedonian, Tajik and Mongolian
        # text with the list of Russian fonts.
    elif lang == "cyr_lid":
        TEXT_CORPUS = f"{FLAGS_webtext_prefix}/cyr_lid.corpus.txt"
        TRAINING_DATA_ARGUMENTS += ["--infrequent_ratio=10000"]
        GENERATE_WORD_BIGRAMS = 0
        WORD_DAWG_SIZE = 1_000_000
        if not FONTS:
            FONTS = RUSSIAN_FONTS

        # South Asian scripts mostly have a lot of different graphemes, so trim
        # down the MEAN_COUNT so as not to get a huge amount of text.
    elif lang in ("asm", "ben"):
        MEAN_COUNT = 15
        WORD_DAWG_FACTOR = 0.15
        if not FONTS:
            FONTS = BENGALI_FONTS
    elif lang in ("bih", "hin", "mar", "nep", "san"):
        MEAN_COUNT = 15
        WORD_DAWG_FACTOR = 0.15
        if not FONTS:
            FONTS = DEVANAGARI_FONTS
    elif lang == "bod":
        MEAN_COUNT = 15
        WORD_DAWG_FACTOR = 0.15
        if not FONTS:
            FONTS = TIBETAN_FONTS
    elif lang == "dzo":
        WORD_DAWG_FACTOR = 0.01
        if not FONTS:
            FONTS = TIBETAN_FONTS
    elif lang == "guj":
        MEAN_COUNT = 15
        WORD_DAWG_FACTOR = 0.15
        if not FONTS:
            FONTS = GUJARATI_FONTS
    elif lang == "kan":
        MEAN_COUNT = 15
        WORD_DAWG_FACTOR = 0.15
        TRAINING_DATA_ARGUMENTS += ["--no_newline_in_output"]
        TEXT2IMAGE_EXTRA_ARGS += ["--char_spacing=0.5"]
        if not FONTS:
            FONTS = KANNADA_FONTS
    elif lang == "mal":
        MEAN_COUNT = 15
        WORD_DAWG_FACTOR = 0.15
        TRAINING_DATA_ARGUMENTS += ["--no_newline_in_output"]
        TEXT2IMAGE_EXTRA_ARGS += ["--char_spacing=0.5"]
        if not FONTS:
            FONTS = MALAYALAM_FONTS
    elif lang == "ori":
        WORD_DAWG_FACTOR = 0.01
        if not FONTS:
            FONTS = ORIYA_FONTS
    elif lang == "pan":
        MEAN_COUNT = 15
        WORD_DAWG_FACTOR = 0.01
        if not FONTS:
            FONTS = PUNJABI_FONTS
    elif lang == "sin":
        MEAN_COUNT = 15
        WORD_DAWG_FACTOR = 0.01
        if not FONTS:
            FONTS = SINHALA_FONTS
    elif lang == "tam":
        MEAN_COUNT = 30
        WORD_DAWG_FACTOR = 0.15
        TRAINING_DATA_ARGUMENTS += ["--no_newline_in_output"]
        TEXT2IMAGE_EXTRA_ARGS += ["--char_spacing=0.5"]
        if not FONTS:
            FONTS = TAMIL_FONTS
    elif lang == "tel":
        MEAN_COUNT = 15
        WORD_DAWG_FACTOR = 0.15
        TRAINING_DATA_ARGUMENTS += ["--no_newline_in_output"]
        TEXT2IMAGE_EXTRA_ARGS += ["--char_spacing=0.5"]
        if not FONTS:
            FONTS = TELUGU_FONTS

        # SouthEast Asian scripts.
    elif lang == "jav_java":
        MEAN_COUNT = 15
        WORD_DAWG_FACTOR = 0.15
        TRAINING_DATA_ARGUMENTS += ["--infrequent_ratio=10000"]
        if not FONTS:
            FONTS = JAVANESE_FONTS
    elif lang == "khm":
        MEAN_COUNT = 15
        WORD_DAWG_FACTOR = 0.15
        TRAINING_DATA_ARGUMENTS += ["--infrequent_ratio=10000"]
        if not FONTS:
            FONTS = KHMER_FONTS
    elif lang == "lao":
        MEAN_COUNT = 15
        WORD_DAWG_FACTOR = 0.15
        TRAINING_DATA_ARGUMENTS += ["--infrequent_ratio=10000"]
        if not FONTS:
            FONTS = LAOTHIAN_FONTS
    elif lang == "mya":
        MEAN_COUNT = 12
        WORD_DAWG_FACTOR = 0.15
        TRAINING_DATA_ARGUMENTS += ["--infrequent_ratio=10000"]
        if not FONTS:
            FONTS = BURMESE_FONTS
    elif lang == "tha":
        MEAN_COUNT = 30
        WORD_DAWG_FACTOR = 0.01
        TRAINING_DATA_ARGUMENTS += ["--infrequent_ratio=10000"]
        FILTER_ARGUMENTS += ["--segmenter_lang=tha"]
        TRAINING_DATA_ARGUMENTS += ["--no_space_in_output", "--desired_bigrams="]
        AMBIGS_FILTER_DENOMINATOR = "1000"
        LEADING = 48
        if not FONTS:
            FONTS = THAI_FONTS

        # CJK
    elif lang == "chi_sim":
        MEAN_COUNT = 15
        PUNC_DAWG_FACTOR = 0.015
        WORD_DAWG_FACTOR = 0.015
        GENERATE_WORD_BIGRAMS = 0
        TRAINING_DATA_ARGUMENTS += ["--infrequent_ratio=10000"]
        TRAINING_DATA_ARGUMENTS += ["--no_space_in_output", "--desired_bigrams="]
        FILTER_ARGUMENTS += ["--charset_filter=chi_sim", "--segmenter_lang=chi_sim"]
        if not FONTS:
            FONTS = CHI_SIM_FONTS
    elif lang == "chi_tra":
        MEAN_COUNT = 15
        WORD_DAWG_FACTOR = 0.015
        GENERATE_WORD_BIGRAMS = 0
        TRAINING_DATA_ARGUMENTS += ["--infrequent_ratio=10000"]
        TRAINING_DATA_ARGUMENTS += ["--no_space_in_output", "--desired_bigrams="]
        FILTER_ARGUMENTS += ["--charset_filter=chi_tr", "--segmenter_lang=chi_tra"]
        if not FONTS:
            FONTS = CHI_TRA_FONTS
    elif lang == "jpn":
        MEAN_COUNT = 15
        WORD_DAWG_FACTOR = 0.015
        GENERATE_WORD_BIGRAMS = 0
        TRAINING_DATA_ARGUMENTS += ["--infrequent_ratio=10000"]
        TRAINING_DATA_ARGUMENTS += ["--no_space_in_output", "--desired_bigrams="]
        FILTER_ARGUMENTS += ["--charset_filter=jpn", "--segmenter_lang=jpn"]
        if not FONTS:
            FONTS = JPN_FONTS
    elif lang == "kor":
        MEAN_COUNT = 20
        WORD_DAWG_FACTOR = 0.015
        NUMBER_DAWG_FACTOR = 0.05
        TRAINING_DATA_ARGUMENTS += ["--infrequent_ratio=10000"]
        TRAINING_DATA_ARGUMENTS += ["--desired_bigrams="]
        GENERATE_WORD_BIGRAMS = 0
        FILTER_ARGUMENTS += ["--charset_filter=kor", "--segmenter_lang=kor"]
        if not FONTS:
            FONTS = KOREAN_FONTS

        # Middle-Eastern scripts.
    elif lang == "ara":
        if not FONTS:
            FONTS = ARABIC_FONTS
    elif lang == "div":
        if not FONTS:
            FONTS = THAANA_FONTS
    elif lang in ("fas", "pus", "snd", "uig", "urd"):
        if not FONTS:
            FONTS = PERSIAN_FONTS
    elif lang in ("heb", "yid"):
        NUMBER_DAWG_FACTOR = 0.05
        WORD_DAWG_FACTOR = 0.08
        if not FONTS:
            FONTS = HEBREW_FONTS
    elif lang == "syr":
        if not FONTS:
            FONTS = SYRIAC_FONTS

        # Other scripts.
    elif lang in ("amh", "tir"):
        if not FONTS:
            FONTS = AMHARIC_FONTS
    elif lang == "chr":
        if not FONTS:
            FONTS = [*NORTH_AMERICAN_ABORIGINAL_FONTS, "Noto Sans Cherokee"]
    elif lang == "ell":
        NUMBER_DAWG_FACTOR = 0.05
        WORD_DAWG_FACTOR = 0.08
        if not FONTS:
            FONTS = GREEK_FONTS
    elif lang == "grc":
        if not EXPOSURES:
            EXPOSURES = "-3 -2 -1 0 1 2 3".split()
        if not FONTS:
            FONTS = ANCIENT_GREEK_FONTS
    elif lang == "hye":
        if not FONTS:
            FONTS = ARMENIAN_FONTS
    elif lang == "iku":
        if not FONTS:
            FONTS = NORTH_AMERICAN_ABORIGINAL_FONTS
    elif lang == "kat":
        if not FONTS:
            FONTS = GEORGIAN_FONTS
    elif lang == "kat_old":
        TEXT_CORPUS = f"{FLAGS_webtext_prefix}/kat.corpus.txt"
        if not FONTS:
            FONTS = OLD_GEORGIAN_FONTS
    elif lang == "kir":
        if not FONTS:
            FONTS = KYRGYZ_FONTS
        TRAINING_DATA_ARGUMENTS += ["--infrequent_ratio=100"]
    elif lang == "kmr":
        if not FONTS:
            FONTS = LATIN_FONTS
    elif lang == "kur_ara":
        if not FONTS:
            FONTS = KURDISH_FONTS
    else:
        raise ValueError(f"Error: {lang} is not a valid language code")

    FLAGS_mean_count = int(os.environ.get("FLAGS_mean_count", -1))
    if FLAGS_mean_count > 0:
        TRAINING_DATA_ARGUMENTS += [f"--mean_count={FLAGS_mean_count}"]
    elif not MEAN_COUNT:
        TRAINING_DATA_ARGUMENTS += [f"--mean_count={MEAN_COUNT}"]

    # Default to Latin fonts if none have been set
    if not FONTS:
        FONTS = LATIN_FONTS

    # Default to 0 exposure if it hasn't been set
    if not EXPOSURES:
        EXPOSURES = [0]
    # Set right-to-left and normalization mode.
    if lang in (
            "ara",
            "div",
            "fas",
            "pus",
            "snd",
            "syr",
            "uig",
            "urd",
            "kur_ara",
            "heb",
            "yid",
    ):
        LANG_IS_RTL = True
        NORM_MODE = 2
    elif lang in (
            "asm",
            "ben",
            "bih",
            "hin",
            "mar",
            "nep",
            "guj",
            "kan",
            "mal",
            "tam",
            "tel",
            "pan",
            "dzo",
            "sin",
            "san",
            "bod",
            "ori",
            "khm",
            "mya",
            "tha",
            "lao",
            "jav ",
            "jav_java",
    ):
        LANG_IS_RTL = False
        NORM_MODE = 2
    else:
        LANG_IS_RTL = False
        NORM_MODE = 1

    vars_to_transfer = {
        'ambigs_filter_denominator': AMBIGS_FILTER_DENOMINATOR,
        'bigram_dawg_factor': BIGRAM_DAWG_FACTOR,
        'exposures': EXPOSURES,
        'filter_arguments': FILTER_ARGUMENTS,
        'fonts': FONTS,
        'fragments_disabled': FRAGMENTS_DISABLED,
        'generate_word_bigrams': GENERATE_WORD_BIGRAMS,
        'lang_is_rtl': LANG_IS_RTL,
        'leading': LEADING,
        'mean_count': MEAN_COUNT,
        'mix_lang': MIX_LANG,
        'norm_mode': NORM_MODE,
        'number_dawg_factor': NUMBER_DAWG_FACTOR,
        'punc_dawg_factor': PUNC_DAWG_FACTOR,
        'run_shape_clustering': RUN_SHAPE_CLUSTERING,
        'text2image_extra_args': TEXT2IMAGE_EXTRA_ARGS,
        'text_corpus': TEXT_CORPUS,
        'training_data_arguments': TRAINING_DATA_ARGUMENTS,
        'word_dawg_factor': WORD_DAWG_FACTOR,
        'word_dawg_size': WORD_DAWG_SIZE,
        'wordlist2dawg_arguments': WORDLIST2DAWG_ARGUMENTS,
    }

    for attr, value in vars_to_transfer.items():
        if hasattr(ctx, attr):
            if getattr(ctx, attr) != value:
                log.debug(f"{attr} = {value} (was {getattr(ctx, attr)})")
                setattr(ctx, attr, value)
            else:
                log.debug(f"{attr} = {value} (set on cmdline)")
        else:
            log.debug(f"{attr} = {value}")
            setattr(ctx, attr, value)

    return ctx
