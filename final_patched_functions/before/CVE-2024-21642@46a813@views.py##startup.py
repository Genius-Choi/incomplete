def startup(
    url="",
    data=None,
    data_loader=None,
    name=None,
    data_id=None,
    context_vars=None,
    ignore_duplicate=True,
    allow_cell_edits=True,
    inplace=False,
    drop_index=False,
    precision=2,
    show_columns=None,
    hide_columns=None,
    optimize_dataframe=False,
    column_formats=None,
    nan_display=None,
    sort=None,
    locked=None,
    background_mode=None,
    range_highlights=None,
    app_root=None,
    is_proxy=None,
    vertical_headers=False,
    hide_shutdown=None,
    column_edit_options=None,
    auto_hide_empty_columns=False,
    highlight_filter=False,
    hide_header_editor=None,
    lock_header_menu=None,
    hide_header_menu=None,
    hide_main_menu=None,
    hide_column_menus=None,
    enable_custom_filters=None,
    force_save=True,
):
    """
    Loads and stores data globally
     - If data has indexes then it will lock save those columns as locked on the front-end
     - If data has column named index it will be dropped so that it won't collide with row numbering (dtale_index)
     - Create location in memory for storing settings which can be manipulated from the front-end (sorts, filter, ...)

    :param url: the base URL that D-Tale is running from to be referenced in redirects to shutdown
    :param data: :class:`pandas:pandas.DataFrame` or :class:`pandas:pandas.Series`
    :param data_loader: function which returns :class:`pandas:pandas.DataFrame`
    :param name: string label to apply to your session
    :param data_id: integer id assigned to a piece of data viewable in D-Tale, if this is populated then it will
                    override the data at that id
    :param context_vars: a dictionary of the variables that will be available for use in user-defined expressions,
                         such as filters
    :type context_vars: dict, optional
    :param ignore_duplicate: if set to True this will not test whether this data matches any previously loaded to D-Tale
    :param allow_cell_edits: If false, this will not allow users to edit cells directly in their D-Tale grid
    :type allow_cell_edits: bool, optional
    :param inplace: If true, this will call `reset_index(inplace=True)` on the dataframe used as a way to save memory.
                    Otherwise this will create a brand new dataframe, thus doubling memory but leaving the dataframe
                    input unchanged.
    :type inplace: bool, optional
    :param drop_index: If true, this will drop any pre-existing index on the dataframe input.
    :type drop_index: bool, optional
    :param precision: The default precision to display for float data in D-Tale grid
    :type precision: int, optional
    :param show_columns: Columns to show on load, hide all others
    :type show_columns: list, optional
    :param hide_columns: Columns to hide on load
    :type hide_columns: list, optional
    :param optimize_dataframe: this will convert string columns with less certain then a certain number of distinct
                              values into categories
    :type optimize_dataframe: boolean
    :param column_formats: The formatting to apply to certain columns on the front-end
    :type column_formats: dict, optional
    :param sort: The sort to apply to the data on startup (EX: [("col1", "ASC"), ("col2", "DESC"),...])
    :type sort: list[tuple], optional
    :param locked: Columns to lock to the left of your grid on load
    :type locked: list, optional
    :param background_mode: Different background highlighting modes available on the frontend. Possible values are:
                            - heatmap-all: turn on heatmap for all numeric columns where the colors are determined by
                                           the range of values over all numeric columns combined
                            - heatmap-col: turn on heatmap for all numeric columns where the colors are determined by
                                           the range of values in the column
                            - heatmap-col-[column name]: turn on heatmap highlighting for a specific column
                            - dtypes: highlight columns based on it's data type
                            - missing: highlight any missing values (np.nan, empty strings, strings of all spaces)
                            - outliers: highlight any outliers
                            - range: highlight values for any matchers entered in the "range_highlights" option
                            - lowVariance: highlight values with a low variance
    :type background_mode: string, optional
    :param range_highlights: Definitions for equals, less-than or greater-than ranges for individual (or all) columns
                             which apply different background colors to cells which fall in those ranges.
    :type range_highlights: dict, optional
    :param vertical_headers: if True, then rotate column headers vertically
    :type vertical_headers: boolean, optional
    :param column_edit_options: The options to allow on the front-end when editing a cell for the columns specified
    :type column_edit_options: dict, optional
    :param auto_hide_empty_columns: if True, then auto-hide any columns on the front-end that are comprised entirely of
                                    NaN values
    :type auto_hide_empty_columns: boolean, optional
    :param highlight_filter: if True, then highlight rows on the frontend which will be filtered when applying a filter
                             rather than hiding them from the dataframe
    :type highlight_filter: boolean, optional
    """

    if (
        data_loader is None and data is None
    ):  # scenario where we'll force users to upload a CSV/TSV
        return DtaleData("1", url, is_proxy=is_proxy, app_root=app_root)

    if data_loader is not None:
        data = data_loader()
        if isinstance(data, string_types) and global_state.contains(data):
            return DtaleData(data, url, is_proxy=is_proxy, app_root=app_root)
        elif (
            data is None and global_state.is_arcticdb
        ):  # send user to the library/symbol selection screen
            return DtaleData(None, url, is_proxy=is_proxy, app_root=app_root)

    if global_state.is_arcticdb and isinstance(data, string_types):
        data_id = data
        data_id_segs = data_id.split("|")
        if len(data_id_segs) < 2:
            if not global_state.store.lib:
                raise ValueError(
                    (
                        "When specifying a data identifier for ArcticDB it must be comprised of a library and a symbol."
                        "Use the following format: [library]|[symbol]"
                    )
                )
            data_id = "{}|{}".format(global_state.store.lib.name, data_id)
        global_state.new_data_inst(data_id)
        instance = global_state.store.get(data_id)
        data = instance.base_df
        ret_data = startup(
            url=url,
            data=data,
            data_id=data_id,
            force_save=False,
            name=name,
            context_vars=context_vars,
            ignore_duplicate=ignore_duplicate,
            allow_cell_edits=allow_cell_edits,
            precision=precision,
            show_columns=show_columns,
            hide_columns=hide_columns,
            column_formats=column_formats,
            nan_display=nan_display,
            sort=sort,
            locked=locked,
            background_mode=background_mode,
            range_highlights=range_highlights,
            app_root=app_root,
            is_proxy=is_proxy,
            vertical_headers=vertical_headers,
            hide_shutdown=hide_shutdown,
            column_edit_options=column_edit_options,
            auto_hide_empty_columns=auto_hide_empty_columns,
            highlight_filter=highlight_filter,
            hide_header_editor=hide_header_editor,
            lock_header_menu=lock_header_menu,
            hide_header_menu=hide_header_menu,
            hide_main_menu=hide_main_menu,
            hide_column_menus=hide_column_menus,
            enable_custom_filters=enable_custom_filters,
        )
        startup_code = (
            "from arcticdb import Arctic\n"
            "from arcticdb.version_store._store import VersionedItem\n\n"
            "conn = Arctic('{uri}')\n"
            "lib = conn.get_library('{library}')\n"
            "df = lib.read('{symbol}')\n"
            "if isinstance(data, VersionedItem):\n"
            "\tdf = df.data\n"
        ).format(
            uri=global_state.store.uri,
            library=global_state.store.lib.name,
            symbol=data_id,
        )
        curr_settings = global_state.get_settings(data_id)
        global_state.set_settings(
            data_id, dict_merge(curr_settings, dict(startup_code=startup_code))
        )
        return ret_data

    if data is not None:
        data = handle_koalas(data)
        valid_types = (
            pd.DataFrame,
            pd.Series,
            pd.DatetimeIndex,
            pd.MultiIndex,
            xr.Dataset,
            np.ndarray,
            list,
            dict,
        )
        if not isinstance(data, valid_types):
            raise Exception(
                (
                    "data loaded must be one of the following types: pandas.DataFrame, pandas.Series, "
                    "pandas.DatetimeIndex, pandas.MultiIndex, xarray.Dataset, numpy.array, numpy.ndarray, list, dict"
                )
            )

        if isinstance(data, xr.Dataset):
            df = convert_xarray_to_dataset(data)
            instance = startup(
                url,
                df,
                name=name,
                data_id=data_id,
                context_vars=context_vars,
                ignore_duplicate=ignore_duplicate,
                allow_cell_edits=allow_cell_edits,
                precision=precision,
                show_columns=show_columns,
                hide_columns=hide_columns,
                column_formats=column_formats,
                nan_display=nan_display,
                sort=sort,
                locked=locked,
                background_mode=background_mode,
                range_highlights=range_highlights,
                app_root=app_root,
                is_proxy=is_proxy,
                vertical_headers=vertical_headers,
                hide_shutdown=hide_shutdown,
                column_edit_options=column_edit_options,
                auto_hide_empty_columns=auto_hide_empty_columns,
                highlight_filter=highlight_filter,
                hide_header_editor=hide_header_editor,
                lock_header_menu=lock_header_menu,
                hide_header_menu=hide_header_menu,
                hide_main_menu=hide_main_menu,
                hide_column_menus=hide_column_menus,
                enable_custom_filters=enable_custom_filters,
            )

            global_state.set_dataset(instance._data_id, data)
            global_state.set_dataset_dim(instance._data_id, {})
            return instance

        data, curr_index = format_data(data, inplace=inplace, drop_index=drop_index)
        # check to see if this dataframe has already been loaded to D-Tale
        if data_id is None and not ignore_duplicate and not global_state.is_arcticdb:
            check_duplicate_data(data)

        logger.debug(
            "pytest: {}, flask-debug: {}".format(
                running_with_pytest(), running_with_flask_debug()
            )
        )

        if data_id is None:
            data_id = global_state.new_data_inst()
        if global_state.get_settings(data_id) is not None:
            curr_settings = global_state.get_settings(data_id)
            curr_locked = curr_settings.get("locked", [])
            # filter out previous locked columns that don't exist
            curr_locked = [c for c in curr_locked if c in data.columns]
            # add any new columns in index
            curr_locked += [c for c in curr_index if c not in curr_locked]
        else:
            logger.debug(
                "pre-locking index columns ({}) to settings[{}]".format(
                    curr_index, data_id
                )
            )
            curr_locked = locked or curr_index
            global_state.set_metadata(data_id, dict(start=pd.Timestamp("now")))
        global_state.set_name(data_id, name)
        # in the case that data has been updated we will drop any sorts or filter for ease of use
        base_settings = dict(
            indexes=curr_index,
            locked=curr_locked,
            allow_cell_edits=True if allow_cell_edits is None else allow_cell_edits,
            precision=precision,
            columnFormats=column_formats or {},
            backgroundMode=background_mode,
            rangeHighlight=range_highlights,
            verticalHeaders=vertical_headers,
            highlightFilter=highlight_filter,
        )
        base_predefined = predefined_filters.init_filters()
        if base_predefined:
            base_settings["predefinedFilters"] = base_predefined
        if sort:
            base_settings["sortInfo"] = sort
            data = sort_df_for_grid(data, dict(sort=sort))
        if nan_display is not None:
            base_settings["nanDisplay"] = nan_display
        if hide_shutdown is not None:
            base_settings["hide_shutdown"] = hide_shutdown
        if hide_header_editor is not None:
            base_settings["hide_header_editor"] = hide_header_editor
        if lock_header_menu is not None:
            base_settings["lock_header_menu"] = lock_header_menu
        if hide_header_menu is not None:
            base_settings["hide_header_menu"] = hide_header_menu
        if hide_main_menu is not None:
            base_settings["hide_main_menu"] = hide_main_menu
        if hide_column_menus is not None:
            base_settings["hide_column_menus"] = hide_column_menus
        if enable_custom_filters is not None:
            base_settings["enable_custom_filters"] = enable_custom_filters
        if column_edit_options is not None:
            base_settings["column_edit_options"] = column_edit_options
        global_state.set_settings(data_id, base_settings)
        if optimize_dataframe and not global_state.is_arcticdb:
            data = optimize_df(data)
        if force_save or (
            global_state.is_arcticdb and not global_state.contains(data_id)
        ):
            data = data[curr_locked + [c for c in data.columns if c not in curr_locked]]
            global_state.set_data(data_id, data)
        dtypes_data = data
        ranges = None
        if global_state.is_arcticdb:
            instance = global_state.store.get(data_id)
            if not instance.is_large:
                dtypes_data = instance.load_data()
                dtypes_data, _ = format_data(
                    dtypes_data, inplace=inplace, drop_index=drop_index
                )
                ranges = calc_data_ranges(dtypes_data)
                dtypes_data = dtypes_data[
                    curr_locked
                    + [c for c in dtypes_data.columns if c not in curr_locked]
                ]
        dtypes_state = build_dtypes_state(
            dtypes_data, global_state.get_dtypes(data_id) or [], ranges=ranges
        )

        for col in dtypes_state:
            if show_columns and col["name"] not in show_columns:
                col["visible"] = False
                continue
            if hide_columns and col["name"] in hide_columns:
                col["visible"] = False
                continue
            if col["index"] >= 100:
                col["visible"] = False
        if auto_hide_empty_columns and not global_state.is_arcticdb:
            is_empty = data.isnull().all()
            is_empty = list(is_empty[is_empty].index.values)
            for col in dtypes_state:
                if col["name"] in is_empty:
                    col["visible"] = False
        global_state.set_dtypes(data_id, dtypes_state)
        global_state.set_context_variables(
            data_id, build_context_variables(data_id, context_vars)
        )
        if global_state.load_flag(data_id, "enable_custom_filters", False):
            logger.warning(
                (
                    "Custom filtering enabled. Custom filters are vulnerable to code injection attacks, please only "
                    "use in trusted environments."
                )
            )
        return DtaleData(data_id, url, is_proxy=is_proxy, app_root=app_root)
    else:
        raise NoDataLoadedException("No data has been loaded into this D-Tale session!")
