    TfLiteRegistration FakeFusedRegistration() {
      TfLiteRegistration reg = {nullptr};
      reg.custom_name = "fake_fused_op";

      // Different flavors of the delegate kernel's Invoke(), dependent on
      // testing parameters.
      if (fail_delegate_node_invoke_) {
        reg.invoke = [](TfLiteContext* context,
                        TfLiteNode* node) -> TfLiteStatus {
          return kTfLiteError;
        };
      } else {
        reg.invoke = [](TfLiteContext* context,
                        TfLiteNode* node) -> TfLiteStatus {
          // Copy input data to output data.
          const TfLiteTensor* a0;
          const TfLiteTensor* a1;
          if (node->inputs->size == 2) {
            a0 = GetInput(context, node, 0);
            a1 = GetInput(context, node, 1);
          } else {
            a0 = GetInput(context, node, 0);
            a1 = a0;
          }
          TfLiteTensor* out = GetOutput(context, node, 0);
          int num = 1;
          for (int i = 0; i < a0->dims->size; ++i) {
            num *= a0->dims->data[i];
          }
          for (int i = 0; i < num; i++) {
            out->data.f[i] = a0->data.f[i] + a1->data.f[i];
          }
          if (out->buffer_handle != kTfLiteNullBufferHandle) {
            // Make the data stale so that CopyFromBufferHandle can be invoked
            out->data_is_stale = true;
          }
          return kTfLiteOk;
        };
      }

      // Different flavors of the delegate kernel's Prepare(), dependent on
      // testing parameters.
      if (automatic_shape_propagation_) {
        reg.prepare = [](TfLiteContext* context, TfLiteNode* node) {
          // Shapes should already by propagated by the runtime, just need to
          // check.
          const TfLiteTensor* input1 = GetInput(context, node, 0);
          TfLiteTensor* output = GetOutput(context, node, 0);
          const int input_dims_size = input1->dims->size;
          TF_LITE_ENSURE(context, output->dims->size == input_dims_size);
          for (int i = 0; i < input_dims_size; ++i) {
            TF_LITE_ENSURE(context,
                           output->dims->data[i] == input1->dims->data[i]);
          }
          return kTfLiteOk;
        };
      } else if (fail_delegate_node_prepare_) {
        reg.prepare = [](TfLiteContext* context, TfLiteNode* node) {
          return kTfLiteError;
        };
      } else {
        reg.prepare = [](TfLiteContext* context, TfLiteNode* node) {
          // Set output size to input size
          const TfLiteTensor* input1;
          const TfLiteTensor* input2;
          if (node->inputs->size == 2) {
            input1 = GetInput(context, node, 0);
            input2 = GetInput(context, node, 1);
          } else {
            input1 = GetInput(context, node, 0);
            input2 = input1;
          }
          TfLiteTensor* output = GetOutput(context, node, 0);

          TF_LITE_ENSURE_STATUS(context->ResizeTensor(
              context, output, TfLiteIntArrayCopy(input1->dims)));
          return kTfLiteOk;
        };
      }

      return reg;
    }
