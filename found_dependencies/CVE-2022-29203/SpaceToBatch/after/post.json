[
    {
        "line": 5,
        "fullcodeline": "const int input_rank = input_tensor_shape.dims();"
    },
    {
        "line": 6,
        "fullcodeline": "const absl::InlinedVector<int64_t, 4> input_shape ="
    },
    {
        "line": 8,
        "fullcodeline": "const int block_rank = block_shape.size();"
    },
    {
        "line": 10,
        "fullcodeline": "OP_REQUIRES("
    },
    {
        "line": 14,
        "fullcodeline": "absl::Span<const int64_t> remainder_shape(input_shape);"
    },
    {
        "line": 15,
        "fullcodeline": "remainder_shape.remove_prefix(1 + block_rank);"
    },
    {
        "line": 17,
        "fullcodeline": "OP_REQUIRES("
    },
    {
        "line": 26,
        "fullcodeline": "xla::XlaBuilder* b = ctx->builder();"
    },
    {
        "line": 31,
        "fullcodeline": "std::vector<int64_t> padded_shape(input_shape.begin(), input_shape.end());"
    },
    {
        "line": 32,
        "fullcodeline": "int64_t block_num_elems = 1LL;"
    },
    {
        "line": 33,
        "fullcodeline": "padding_config.add_dimensions();  // Don't pad the batch dimension."
    },
    {
        "line": 53,
        "fullcodeline": "OP_REQUIRES(ctx, block_num_elems > 0,"
    },
    {
        "line": 56,
        "fullcodeline": "const int64_t batch_size = input_shape[0];"
    },
    {
        "line": 57,
        "fullcodeline": "const int64_t output_dim ="
    },
    {
        "line": 67,
        "fullcodeline": "xla::XlaOp padded ="
    },
    {
        "line": 79,
        "fullcodeline": "std::vector<int64_t> reshaped_padded_shape(input_rank + block_rank);"
    },
    {
        "line": 80,
        "fullcodeline": "reshaped_padded_shape[0] = batch_size;"
    },
    {
        "line": 91,
        "fullcodeline": "std::copy(remainder_shape.begin(), remainder_shape.end(),"
    },
    {
        "line": 94,
        "fullcodeline": "xla::XlaOp reshaped_padded = xla::Reshape(padded, reshaped_padded_shape);"
    },
    {
        "line": 105,
        "fullcodeline": "std::vector<int64_t> permutation(reshaped_padded_shape.size());"
    },
    {
        "line": 110,
        "fullcodeline": "permutation[block_rank] = 0;"
    },
    {
        "line": 111,
        "fullcodeline": "std::iota(permutation.begin() + 1 + block_rank * 2, permutation.end(),"
    },
    {
        "line": 113,
        "fullcodeline": "xla::XlaOp permuted_reshaped_padded ="
    },
    {
        "line": 126,
        "fullcodeline": "std::vector<int64_t> output_shape(input_rank);"
    },
    {
        "line": 127,
        "fullcodeline": "output_shape[0] = output_dim;"
    },
    {
        "line": 131,
        "fullcodeline": "std::copy(remainder_shape.begin(), remainder_shape.end(),"
    },
    {
        "line": 134,
        "fullcodeline": "xla::XlaOp output = xla::Reshape(permuted_reshaped_padded, output_shape);"
    },
    {
        "line": 135,
        "fullcodeline": "ctx->SetOutput(0, output);"
    },
    {
        "line": 7,
        "fullcodeline": "input_tensor_shape.dim_sizes();"
    },
    {
        "line": 11,
        "fullcodeline": "ctx, input_rank >= 1 + block_rank,"
    },
    {
        "line": 12,
        "fullcodeline": "errors::InvalidArgument(\"input rank should be >= \", 1 + block_rank,"
    },
    {
        "line": 19,
        "fullcodeline": "paddings.shape().rank() == 2 &&"
    },
    {
        "line": 22,
        "fullcodeline": "errors::InvalidArgument(\"paddings should have shape [\", block_rank,"
    },
    {
        "line": 34,
        "fullcodeline": "for (int i = 0; i < block_rank; ++i) {"
    },
    {
        "line": 50,
        "fullcodeline": "for (int i = 0; i < remainder_shape.size(); ++i) {"
    },
    {
        "line": 54,
        "fullcodeline": "errors::InvalidArgument("
    },
    {
        "line": 58,
        "fullcodeline": "MultiplyWithoutOverflow(batch_size, block_num_elems);"
    },
    {
        "line": 59,
        "fullcodeline": "if (output_dim < 0) {"
    },
    {
        "line": 68,
        "fullcodeline": "xla::Pad(input, XlaHelpers::Zero(b, input_dtype), padding_config);"
    },
    {
        "line": 81,
        "fullcodeline": "for (int i = 0; i < block_rank; ++i) {"
    },
    {
        "line": 92,
        "fullcodeline": "reshaped_padded_shape.begin() + 1 + 2 * block_rank);"
    },
    {
        "line": 106,
        "fullcodeline": "for (int i = 0; i < block_rank; ++i) {"
    },
    {
        "line": 112,
        "fullcodeline": "1 + block_rank * 2);"
    },
    {
        "line": 114,
        "fullcodeline": "xla::Transpose(reshaped_padded, permutation);"
    },
    {
        "line": 128,
        "fullcodeline": "for (int i = 0; i < block_rank; ++i) {"
    },
    {
        "line": 132,
        "fullcodeline": "output_shape.begin() + 1 + block_rank);"
    },
    {
        "line": 21,
        "fullcodeline": "2 == xla::ShapeUtil::GetDimension(paddings.shape(), 1),"
    },
    {
        "line": 24,
        "fullcodeline": "xla::ShapeUtil::HumanString(paddings.shape())));"
    },
    {
        "line": 35,
        "fullcodeline": "auto* dim = padding_config.add_dimensions();"
    },
    {
        "line": 36,
        "fullcodeline": "int64_t pad_start = paddings.Get<int64_t>({i, 0});"
    },
    {
        "line": 37,
        "fullcodeline": "int64_t pad_end = paddings.Get<int64_t>({i, 1});"
    },
    {
        "line": 38,
        "fullcodeline": "OP_REQUIRES(ctx, pad_start >= 0 && pad_end >= 0,"
    },
    {
        "line": 40,
        "fullcodeline": "OP_REQUIRES(ctx, block_shape[i] >= 1,"
    },
    {
        "line": 44,
        "fullcodeline": "dim->set_edge_padding_low(pad_start);"
    },
    {
        "line": 45,
        "fullcodeline": "dim->set_edge_padding_high(pad_end);"
    },
    {
        "line": 46,
        "fullcodeline": "padded_shape[1 + i] += pad_start + pad_end;"
    },
    {
        "line": 47,
        "fullcodeline": "block_num_elems = MultiplyWithoutOverflow(block_num_elems, block_shape[i]);"
    },
    {
        "line": 51,
        "fullcodeline": "padding_config.add_dimensions();"
    },
    {
        "line": 60,
        "fullcodeline": "OP_REQUIRES("
    },
    {
        "line": 82,
        "fullcodeline": "OP_REQUIRES(ctx, padded_shape[1 + i] % block_shape[i] == 0,"
    },
    {
        "line": 88,
        "fullcodeline": "reshaped_padded_shape[1 + i * 2] = padded_shape[1 + i] / block_shape[i];"
    },
    {
        "line": 89,
        "fullcodeline": "reshaped_padded_shape[1 + i * 2 + 1] = block_shape[i];"
    },
    {
        "line": 107,
        "fullcodeline": "permutation[i] = 1 + 2 * i + 1;"
    },
    {
        "line": 108,
        "fullcodeline": "permutation[block_rank + 1 + i] = 1 + 2 * i;"
    },
    {
        "line": 129,
        "fullcodeline": "output_shape[1 + i] = padded_shape[1 + i] / block_shape[i];"
    },
    {
        "line": 20,
        "fullcodeline": "block_rank == xla::ShapeUtil::GetDimension(paddings.shape(), 0) &&"
    },
    {
        "line": 39,
        "fullcodeline": "errors::InvalidArgument(\"Paddings must be non-negative\"));"
    },
    {
        "line": 41,
        "fullcodeline": "errors::InvalidArgument("
    },
    {
        "line": 61,
        "fullcodeline": "ctx, output_dim >= 0,"
    },
    {
        "line": 62,
        "fullcodeline": "errors::InvalidArgument(\"Negative output dimension size caused by \""
    },
    {
        "line": 83,
        "fullcodeline": "errors::InvalidArgument(\"padded_shape[\", 1 + i,"
    },
    {
        "line": 84,
        "fullcodeline": "\"]=\", padded_shape[1 + i],"
    }
]