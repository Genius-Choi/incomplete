[
    {
        "line": 5,
        "fullcodeline": "const int input_dims = orig_input_tensor.dims();"
    },
    {
        "line": 11,
        "fullcodeline": "const int block_dims = orig_block_shape.dim_size(0);"
    },
    {
        "line": 29,
        "fullcodeline": "internal::spacetobatch::SubtleMustCopyFlat(orig_block_shape, &block_shape);"
    },
    {
        "line": 30,
        "fullcodeline": "internal::spacetobatch::SubtleMustCopyFlat(orig_paddings, &paddings);"
    },
    {
        "line": 34,
        "fullcodeline": "int removed_prefix_block_dims = 0;"
    },
    {
        "line": 45,
        "fullcodeline": "int removed_suffix_block_dims = 0;"
    },
    {
        "line": 56,
        "fullcodeline": "int64_t block_shape_product = 1;"
    },
    {
        "line": 71,
        "fullcodeline": "const int internal_block_dims ="
    },
    {
        "line": 96,
        "fullcodeline": "const int64_t output_shape = MultiplyWithoutOverflow("
    },
    {
        "line": 103,
        "fullcodeline": "external_output_shape.AddDim(output_shape);"
    },
    {
        "line": 105,
        "fullcodeline": "int64_t input_batch_size = orig_input_tensor.dim_size(0);"
    },
    {
        "line": 111,
        "fullcodeline": "internal_input_shape.AddDim(input_batch_size);"
    },
    {
        "line": 112,
        "fullcodeline": "internal_output_shape.AddDim(input_batch_size * block_shape_product);"
    },
    {
        "line": 136,
        "fullcodeline": "int64_t depth = 1;"
    },
    {
        "line": 143,
        "fullcodeline": "internal_input_shape.AddDim(depth);"
    },
    {
        "line": 147,
        "fullcodeline": "Tensor* output_tensor = nullptr;"
    },
    {
        "line": 6,
        "fullcodeline": "if (!TensorShapeUtils::IsVector(orig_block_shape.shape())) {"
    },
    {
        "line": 27,
        "fullcodeline": "gtl::InlinedVector<int64_t, 4> block_shape;"
    },
    {
        "line": 28,
        "fullcodeline": "gtl::InlinedVector<int64_t, 8> paddings;"
    },
    {
        "line": 35,
        "fullcodeline": "for (; removed_prefix_block_dims < block_dims; ++removed_prefix_block_dims) {"
    },
    {
        "line": 46,
        "fullcodeline": "for (; removed_suffix_block_dims < block_dims - removed_prefix_block_dims;"
    },
    {
        "line": 47,
        "fullcodeline": "++removed_suffix_block_dims) {"
    },
    {
        "line": 57,
        "fullcodeline": "for (int block_dim = 0; block_dim < block_dims; ++block_dim) {"
    },
    {
        "line": 66,
        "fullcodeline": "if (block_shape_product <= 0) {"
    },
    {
        "line": 72,
        "fullcodeline": "block_dims - removed_prefix_block_dims - removed_suffix_block_dims;"
    },
    {
        "line": 73,
        "fullcodeline": "if (internal_block_dims > kMaxSpaceToBatchBlockDims) {"
    },
    {
        "line": 98,
        "fullcodeline": "if (output_shape < 0) {"
    },
    {
        "line": 106,
        "fullcodeline": "for (int block_dim = 0; block_dim < removed_prefix_block_dims; ++block_dim) {"
    },
    {
        "line": 115,
        "fullcodeline": "block_dim < block_dims - removed_suffix_block_dims; ++block_dim) {"
    },
    {
        "line": 137,
        "fullcodeline": "for (int dim = block_dims - removed_suffix_block_dims + 1; dim < input_dims;"
    },
    {
        "line": 138,
        "fullcodeline": "++dim) {"
    },
    {
        "line": 149,
        "fullcodeline": "context->allocate_output(0, external_output_shape, &output_tensor));"
    },
    {
        "line": 170,
        "fullcodeline": "return Status::OK();"
    },
    {
        "line": 12,
        "fullcodeline": "if (orig_input_tensor.dims() < 1 + block_dims) {"
    },
    {
        "line": 17,
        "fullcodeline": "if (!(TensorShapeUtils::IsMatrix(orig_paddings.shape()) &&"
    },
    {
        "line": 36,
        "fullcodeline": "const int dim = removed_prefix_block_dims;"
    },
    {
        "line": 48,
        "fullcodeline": "const int dim = block_dims - 1 - removed_suffix_block_dims;"
    },
    {
        "line": 63,
        "fullcodeline": "block_shape_product ="
    },
    {
        "line": 97,
        "fullcodeline": "orig_input_tensor.dim_size(0), block_shape_product);"
    },
    {
        "line": 107,
        "fullcodeline": "const int64_t size = orig_input_tensor.dim_size(block_dim + 1);"
    },
    {
        "line": 108,
        "fullcodeline": "input_batch_size *= size;"
    },
    {
        "line": 109,
        "fullcodeline": "external_output_shape.AddDim(size);"
    },
    {
        "line": 114,
        "fullcodeline": "for (int block_dim = removed_prefix_block_dims;"
    },
    {
        "line": 116,
        "fullcodeline": "const int64_t pad_start = paddings[2 * block_dim],"
    },
    {
        "line": 117,
        "fullcodeline": "pad_end = paddings[2 * block_dim + 1];"
    },
    {
        "line": 121,
        "fullcodeline": "const int64_t input_size = orig_input_tensor.dim_size(block_dim + 1);"
    },
    {
        "line": 122,
        "fullcodeline": "const int64_t block_shape_value = block_shape[block_dim];"
    },
    {
        "line": 123,
        "fullcodeline": "const int64_t padded_size = input_size + pad_start + pad_end;"
    },
    {
        "line": 130,
        "fullcodeline": "internal_input_shape.AddDim(input_size);"
    },
    {
        "line": 131,
        "fullcodeline": "const int64_t output_size = padded_size / block_shape_value;"
    },
    {
        "line": 132,
        "fullcodeline": "internal_output_shape.AddDim(output_size);"
    },
    {
        "line": 133,
        "fullcodeline": "external_output_shape.AddDim(output_size);"
    },
    {
        "line": 139,
        "fullcodeline": "const int64_t size = orig_input_tensor.dim_size(dim);"
    },
    {
        "line": 140,
        "fullcodeline": "external_output_shape.AddDim(size);"
    },
    {
        "line": 141,
        "fullcodeline": "depth *= size;"
    },
    {
        "line": 7,
        "fullcodeline": "return errors::InvalidArgument(\"block_shape rank should be 1 instead of \","
    },
    {
        "line": 13,
        "fullcodeline": "return errors::InvalidArgument(\"input rank should be >= \", 1 + block_dims,"
    },
    {
        "line": 19,
        "fullcodeline": "2 == orig_paddings.dim_size(1))) {"
    },
    {
        "line": 20,
        "fullcodeline": "return errors::InvalidArgument(\"paddings should have shape [\", block_dims,"
    },
    {
        "line": 37,
        "fullcodeline": "if (paddings[2 * dim] != 0 || paddings[2 * dim + 1] != 0 ||"
    },
    {
        "line": 58,
        "fullcodeline": "if (block_shape[block_dim] < 1) {"
    },
    {
        "line": 64,
        "fullcodeline": "MultiplyWithoutOverflow(block_shape_product, block_shape[block_dim]);"
    },
    {
        "line": 67,
        "fullcodeline": "return errors::InvalidArgument("
    },
    {
        "line": 74,
        "fullcodeline": "return errors::InvalidArgument("
    },
    {
        "line": 82,
        "fullcodeline": "return Status::OK();"
    },
    {
        "line": 99,
        "fullcodeline": "return errors::InvalidArgument("
    },
    {
        "line": 8,
        "fullcodeline": "orig_block_shape.dims());"
    },
    {
        "line": 14,
        "fullcodeline": "\" instead of \", orig_input_tensor.dims());"
    },
    {
        "line": 18,
        "fullcodeline": "block_dims == orig_paddings.dim_size(0) &&"
    },
    {
        "line": 22,
        "fullcodeline": "orig_paddings.shape().DebugString());"
    },
    {
        "line": 38,
        "fullcodeline": "block_shape[dim] != 1) {"
    },
    {
        "line": 49,
        "fullcodeline": "if (paddings[dim * 2] != 0 || paddings[dim * 2 + 1] != 0 ||"
    },
    {
        "line": 50,
        "fullcodeline": "block_shape[dim] != 1) {"
    },
    {
        "line": 101,
        "fullcodeline": "orig_input_tensor.dim_size(0), \" and \", block_shape_product);"
    },
    {
        "line": 118,
        "fullcodeline": "if (pad_start < 0 || pad_end < 0) {"
    },
    {
        "line": 124,
        "fullcodeline": "if (padded_size % block_shape_value != 0) {"
    },
    {
        "line": 59,
        "fullcodeline": "return errors::InvalidArgument("
    },
    {
        "line": 119,
        "fullcodeline": "return errors::InvalidArgument(\"Paddings must be non-negative\");"
    },
    {
        "line": 125,
        "fullcodeline": "return errors::InvalidArgument(\"padded_shape[\", block_dim,"
    }
]