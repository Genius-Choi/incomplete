[
    {
        "line": 3,
        "fullcodeline": "const Tensor& data_t = context->input(0);"
    },
    {
        "line": 4,
        "fullcodeline": "const Tensor& batch_index_t = context->input(1);"
    },
    {
        "line": 5,
        "fullcodeline": "const Tensor& grad_t = context->input(2);"
    },
    {
        "line": 7,
        "fullcodeline": "mutex_lock ml(mu_);"
    },
    {
        "line": 9,
        "fullcodeline": "const int64_t batch_key = context->input(3).scalar<int64_t>()();"
    },
    {
        "line": 57,
        "fullcodeline": "auto desire_it = desired_tensor_to_batch_map_.find(batch_key);"
    },
    {
        "line": 11,
        "fullcodeline": "if (!available_tensors_.emplace(batch_key, grad_t).second) {"
    },
    {
        "line": 58,
        "fullcodeline": "if (desire_it != desired_tensor_to_batch_map_.end()) {"
    },
    {
        "line": 74,
        "fullcodeline": "return OkStatus();"
    },
    {
        "line": 17,
        "fullcodeline": "if (data_t.NumElements() > 0) {"
    },
    {
        "line": 22,
        "fullcodeline": "std::unordered_set<int64_t> missing_tensors;"
    },
    {
        "line": 60,
        "fullcodeline": "auto batch_it = available_batches_.find(desire_it->second);"
    },
    {
        "line": 65,
        "fullcodeline": "batch_it->second.missing_tensors.erase(batch_key);"
    },
    {
        "line": 12,
        "fullcodeline": "return errors::InvalidArgument(\"Two runs with the same batch key.\");"
    },
    {
        "line": 24,
        "fullcodeline": "batch_index_t.shaped<int64_t, 2>({batch_index_t.dim_size(0), 3});"
    },
    {
        "line": 25,
        "fullcodeline": "for (int i = 0; i < batch_index_t.dim_size(0); ++i) {"
    },
    {
        "line": 31,
        "fullcodeline": "if (missing_tensors.empty()) {"
    },
    {
        "line": 50,
        "fullcodeline": "output_shape.set_dim(0, 0);"
    },
    {
        "line": 51,
        "fullcodeline": "Tensor* output = nullptr;"
    },
    {
        "line": 62,
        "fullcodeline": "if (batch_it == available_batches_.end()) {"
    },
    {
        "line": 18,
        "fullcodeline": "if (batch_index_t.NumElements() == 0) {"
    },
    {
        "line": 26,
        "fullcodeline": "const int64_t batch_key = batch_index(i, 0);"
    },
    {
        "line": 49,
        "fullcodeline": "TensorShape output_shape(grad_t.shape());"
    },
    {
        "line": 52,
        "fullcodeline": "TF_RETURN_IF_ERROR(context->allocate_output(0, output_shape, &output));"
    },
    {
        "line": 19,
        "fullcodeline": "return errors::InvalidArgument("
    },
    {
        "line": 32,
        "fullcodeline": "return OutputBatch(context, done);"
    },
    {
        "line": 34,
        "fullcodeline": "if (!available_batches_"
    },
    {
        "line": 37,
        "fullcodeline": "return errors::InvalidArgument("
    },
    {
        "line": 63,
        "fullcodeline": "return errors::InvalidArgument(\"Batch no longer exists.\");"
    },
    {
        "line": 70,
        "fullcodeline": "OutputBatch(batch_it->second.context, batch_it->second.done));"
    },
    {
        "line": 27,
        "fullcodeline": "if (available_tensors_.find(batch_key) == available_tensors_.end()) {"
    },
    {
        "line": 28,
        "fullcodeline": "missing_tensors.emplace(batch_key);"
    },
    {
        "line": 35,
        "fullcodeline": ".emplace(batch_key, Batch{missing_tensors, context, done})"
    },
    {
        "line": 41,
        "fullcodeline": "if (!desired_tensor_to_batch_map_.emplace(i, batch_key).second) {"
    },
    {
        "line": 42,
        "fullcodeline": "return errors::InvalidArgument("
    }
]