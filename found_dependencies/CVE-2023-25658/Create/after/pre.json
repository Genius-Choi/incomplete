[
    {
        "line": 12,
        "fullcodeline": "gpu::RnnDescriptor rnn_desc = CreateRnnDescriptor();"
    },
    {
        "line": 13,
        "fullcodeline": "cudnnRNNAlgo_t rnn_algo = ToCudnnRNNAlgo(algorithm_config.algorithm());"
    },
    {
        "line": 16,
        "fullcodeline": "auto proj_size = hidden_size;"
    },
    {
        "line": 17,
        "fullcodeline": "hidden_size = std::max(hidden_size, cell_size);"
    },
    {
        "line": 27,
        "fullcodeline": "bool allow_tensor_ops = data_type == CUDNN_DATA_HALF;"
    },
    {
        "line": 30,
        "fullcodeline": "bool use_tensor_ops ="
    },
    {
        "line": 43,
        "fullcodeline": "cudnnMathType_t math_type ="
    },
    {
        "line": 28,
        "fullcodeline": "if (data_type == CUDNN_DATA_FLOAT)"
    },
    {
        "line": 31,
        "fullcodeline": "algorithm_config.algorithm().has_value()"
    },
    {
        "line": 34,
        "fullcodeline": "if (use_tensor_ops && !allow_tensor_ops) {"
    },
    {
        "line": 44,
        "fullcodeline": "use_tensor_ops ? CUDNN_TENSOR_OP_MATH : CUDNN_DEFAULT_MATH;"
    },
    {
        "line": 63,
        "fullcodeline": "RETURN_IF_CUDNN_ERROR(cudnnSetRNNDescriptor_v6("
    },
    {
        "line": 69,
        "fullcodeline": "CHECK_CUDNN_OK(cudnnSetRNNMatrixMathType(rnn_desc.get(), math_type));"
    },
    {
        "line": 71,
        "fullcodeline": "if (proj_size < hidden_size) {"
    },
    {
        "line": 86,
        "fullcodeline": "tsl::StatusOr<PersistentRnnPlan> rnn_plan_wrapper;"
    },
    {
        "line": 88,
        "fullcodeline": "if (rnn_algo == CUDNN_RNN_ALGO_PERSIST_DYNAMIC) {"
    },
    {
        "line": 110,
        "fullcodeline": "return CudnnRnnDescriptor(cudnn, std::move(rnn_desc), std::move(rnn_plan),"
    },
    {
        "line": 29,
        "fullcodeline": "allow_tensor_ops = tsl::tensor_float_32_execution_enabled();"
    },
    {
        "line": 32,
        "fullcodeline": "? algorithm_config.algorithm()->tensor_ops_enabled()"
    },
    {
        "line": 64,
        "fullcodeline": "cudnn.handle(), /*rnnDesc=*/rnn_desc.get(),"
    },
    {
        "line": 66,
        "fullcodeline": "/*dropoutDesc=*/dropout_desc.handle(), /*inputMode=*/input_mode,"
    },
    {
        "line": 89,
        "fullcodeline": "CHECK_GE(batch_size, 0);"
    },
    {
        "line": 90,
        "fullcodeline": "rnn_plan_wrapper ="
    },
    {
        "line": 114,
        "fullcodeline": "std::move(dropout_desc), std::move(params_desc));"
    },
    {
        "line": 35,
        "fullcodeline": "return tsl::Status(tsl::error::INVALID_ARGUMENT,"
    },
    {
        "line": 72,
        "fullcodeline": "RETURN_IF_CUDNN_ERROR(cudnnSetRNNProjectionLayers("
    },
    {
        "line": 82,
        "fullcodeline": "cudnnSetRNNPaddingMode(rnn_desc.get(), CUDNN_RNN_PADDED_IO_ENABLED));"
    },
    {
        "line": 91,
        "fullcodeline": "CreatePersistentRnnPlan(rnn_desc.get(), batch_size, data_type);"
    },
    {
        "line": 73,
        "fullcodeline": "cudnn.handle(), /*rnnDesc=*/rnn_desc.get(),"
    },
    {
        "line": 92,
        "fullcodeline": "if (!rnn_plan_wrapper.ok()) {"
    },
    {
        "line": 93,
        "fullcodeline": "return tsl::StatusOr<CudnnRnnDescriptor>(rnn_plan_wrapper.status());"
    },
    {
        "line": 95,
        "fullcodeline": "rnn_plan = std::move(rnn_plan_wrapper).value();"
    },
    {
        "line": 97,
        "fullcodeline": "cudnnSetPersistentRNNPlan(rnn_desc.get(), rnn_plan.get()));"
    }
]