[
    {
        "line": 6,
        "fullcodeline": "auto inputs_cleanup = gtl::MakeCleanup([&inputs, &output_tensors] {"
    },
    {
        "line": 17,
        "fullcodeline": "size_t total_inputs_size = 0;"
    },
    {
        "line": 50,
        "fullcodeline": "TF_RETURN_IF_ERROR(EvaluateNode(node, inputs, &output_tensors));"
    },
    {
        "line": 55,
        "fullcodeline": "outputs->resize(output_tensors.size());"
    },
    {
        "line": 51,
        "fullcodeline": "if (output_tensors.empty()) {"
    },
    {
        "line": 56,
        "fullcodeline": "for (size_t i = 0; i < output_tensors.size(); i++) {"
    },
    {
        "line": 74,
        "fullcodeline": "return Status::OK();"
    },
    {
        "line": 19,
        "fullcodeline": "const TensorId input_tensor = ParseTensorName(input);"
    },
    {
        "line": 24,
        "fullcodeline": "const NodeDef* input_node = node_map_->GetNode(input);"
    },
    {
        "line": 30,
        "fullcodeline": "TF_RETURN_IF_ERROR(CheckAttrExists(*input_node, \"value\"));"
    },
    {
        "line": 31,
        "fullcodeline": "const TensorProto& raw_val = input_node->attr().at(\"value\").tensor();"
    },
    {
        "line": 39,
        "fullcodeline": "Tensor* value = new Tensor(raw_val.dtype(), raw_val.tensor_shape());"
    },
    {
        "line": 46,
        "fullcodeline": "inputs.emplace_back(value);"
    },
    {
        "line": 47,
        "fullcodeline": "total_inputs_size += value->TotalBytes();"
    },
    {
        "line": 57,
        "fullcodeline": "string node_name = OptimizedNodeName(node, \"-folded\");"
    },
    {
        "line": 20,
        "fullcodeline": "if (input_tensor.index() < 0) {"
    },
    {
        "line": 25,
        "fullcodeline": "if (!IsReallyConstant(*input_node)) {"
    },
    {
        "line": 32,
        "fullcodeline": "if (raw_val.dtype() == DT_INVALID) {"
    },
    {
        "line": 40,
        "fullcodeline": "if (!value->FromProto(raw_val)) {"
    },
    {
        "line": 52,
        "fullcodeline": "return Status(error::INVALID_ARGUMENT, \"Expected at least one output.\");"
    },
    {
        "line": 58,
        "fullcodeline": "if (output_tensors.size() > 1) {"
    },
    {
        "line": 41,
        "fullcodeline": "delete (value);"
    },
    {
        "line": 59,
        "fullcodeline": "node_name = strings::StrCat(node_name, \"-\", i);"
    },
    {
        "line": 62,
        "fullcodeline": "Status s = CreateNodeDef(node_name, output_tensors[i], &outputs->at(i),"
    },
    {
        "line": 26,
        "fullcodeline": "return Status(error::INVALID_ARGUMENT,"
    },
    {
        "line": 33,
        "fullcodeline": "return Status("
    },
    {
        "line": 42,
        "fullcodeline": "return errors::InvalidArgument(\"Unable to make Tensor from proto for \","
    },
    {
        "line": 64,
        "fullcodeline": "if (!s.ok()) {"
    },
    {
        "line": 71,
        "fullcodeline": "outputs->at(i) = NodeDef();"
    },
    {
        "line": 27,
        "fullcodeline": "strings::StrCat(\"Can't fold \", node.name(), \", its \", input,"
    },
    {
        "line": 35,
        "fullcodeline": "strings::StrCat(\"A tensor in the input node, with TensorId of \","
    },
    {
        "line": 43,
        "fullcodeline": "node.name(), \" with shape \","
    },
    {
        "line": 44,
        "fullcodeline": "raw_val.tensor_shape().DebugString());"
    },
    {
        "line": 65,
        "fullcodeline": "*result_too_large = true;"
    },
    {
        "line": 36,
        "fullcodeline": "input_tensor.ToString(),"
    }
]