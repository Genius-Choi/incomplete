[
    {
        "line": 13,
        "fullcodeline": "bool prepare_original_plan = false;"
    },
    {
        "line": 32,
        "fullcodeline": "int last_exec_plan_index_prepared = 0;"
    },
    {
        "line": 33,
        "fullcodeline": "TF_LITE_ENSURE_STATUS("
    },
    {
        "line": 36,
        "fullcodeline": "next_execution_plan_index_to_prepare_ = last_exec_plan_index_prepared + 1;"
    },
    {
        "line": 39,
        "fullcodeline": "TF_LITE_ENSURE_STATUS(memory_planner_->ExecuteAllocations("
    },
    {
        "line": 60,
        "fullcodeline": "next_execution_plan_index_to_plan_allocation_ ="
    },
    {
        "line": 2,
        "fullcodeline": "if (!memory_planner_) {"
    },
    {
        "line": 14,
        "fullcodeline": "if (!pre_delegation_execution_plan_.empty()) {"
    },
    {
        "line": 34,
        "fullcodeline": "PrepareOpsStartingAt(next_execution_plan_index_to_prepare_,"
    },
    {
        "line": 47,
        "fullcodeline": "for (int i = 0; i < custom_allocations_.size(); ++i) {"
    },
    {
        "line": 61,
        "fullcodeline": "last_exec_plan_index_prepared + 1;"
    },
    {
        "line": 3,
        "fullcodeline": "memory_planner_.reset(new ArenaPlanner("
    },
    {
        "line": 6,
        "fullcodeline": "memory_planner_->PlanAllocations();"
    },
    {
        "line": 24,
        "fullcodeline": "int last_original_exec_plan_index_prepared = 0;"
    },
    {
        "line": 25,
        "fullcodeline": "TF_LITE_ENSURE_STATUS(PrepareOpsStartingAt("
    },
    {
        "line": 28,
        "fullcodeline": "next_original_execution_plan_index_to_prepare_ ="
    },
    {
        "line": 48,
        "fullcodeline": "auto index_and_alloc = custom_allocations_[i];"
    },
    {
        "line": 49,
        "fullcodeline": "TfLiteTensor* tensor_at_index = tensor(index_and_alloc.first);"
    },
    {
        "line": 50,
        "fullcodeline": "const auto& alloc = index_and_alloc.second;"
    },
    {
        "line": 51,
        "fullcodeline": "TF_LITE_ENSURE_EQ(context(), tensor_at_index->allocation_type,"
    },
    {
        "line": 15,
        "fullcodeline": "for (int i = 0; i < delegates_applied_.size(); ++i) {"
    },
    {
        "line": 29,
        "fullcodeline": "last_original_exec_plan_index_prepared + 1;"
    },
    {
        "line": 53,
        "fullcodeline": "if (alloc.bytes < tensor_at_index->bytes) {"
    },
    {
        "line": 4,
        "fullcodeline": "&context_, std::unique_ptr<GraphInfo>(new InterpreterInfo(this)),"
    },
    {
        "line": 54,
        "fullcodeline": "ReportError(\"Custom allocation is too small for tensor idx: %d\","
    },
    {
        "line": 16,
        "fullcodeline": "if ((delegates_applied_[i]->flags &"
    },
    {
        "line": 18,
        "fullcodeline": "prepare_original_plan = true;"
    }
]