[
    {
        "line": 2,
        "fullcodeline": "num_gpus_ = GetNumAvailableGPUs();"
    },
    {
        "line": 3,
        "fullcodeline": "LOG(INFO) << \"Number of GPUs: \" << num_gpus_;"
    },
    {
        "line": 4,
        "fullcodeline": "item_ = &item;"
    },
    {
        "line": 5,
        "fullcodeline": "graph_ = item.graph;"
    },
    {
        "line": 6,
        "fullcodeline": "LOG(INFO) << \"Original graph size: \" << graph_.node_size();"
    },
    {
        "line": 27,
        "fullcodeline": "const std::set<string> apply_gradients_ops = {\"ApplyGradientDescent\","
    },
    {
        "line": 48,
        "fullcodeline": "auto div_const_node = AddNodeDivConst();"
    },
    {
        "line": 49,
        "fullcodeline": "all_nodes_.insert(std::make_pair(div_const_node->name(), div_const_node));"
    },
    {
        "line": 50,
        "fullcodeline": "std::map<string, int> gradient_pos = {{\"ApplyGradientDescent\", 2},"
    },
    {
        "line": 73,
        "fullcodeline": "LOG(INFO) << \"Graph size after adding div nodes: \" << all_nodes_.size();"
    },
    {
        "line": 76,
        "fullcodeline": "TF_RETURN_IF_ERROR(ComputeTransitiveFanin(graph_, item.fetch, &train_nodes));"
    },
    {
        "line": 77,
        "fullcodeline": "LOG(INFO) << \"Number of training nodes: \" << train_nodes.size();"
    },
    {
        "line": 93,
        "fullcodeline": "LOG(INFO) << \"Number of input nodes: \" << input_nodes.size();"
    },
    {
        "line": 95,
        "fullcodeline": "std::set<string> dont_replicate_nodes;"
    },
    {
        "line": 116,
        "fullcodeline": "LOG(INFO) << \"Number of replica nodes: \" << replica_nodes_.size();"
    },
    {
        "line": 123,
        "fullcodeline": "LOG(INFO) << \"Number of shared nodes: \" << shared_nodes_.size();"
    },
    {
        "line": 7,
        "fullcodeline": "if (item.fetch.empty()) {"
    },
    {
        "line": 11,
        "fullcodeline": "if (item.MainVariables().empty()) {"
    },
    {
        "line": 38,
        "fullcodeline": "for (int i = 0; i < graph_.node_size(); i++) {"
    },
    {
        "line": 124,
        "fullcodeline": "return Status::OK();"
    },
    {
        "line": 16,
        "fullcodeline": "VLOG(1) << \"Init node: \" << init;"
    },
    {
        "line": 20,
        "fullcodeline": "VLOG(1) << \"Fetch node: \" << fetch;"
    },
    {
        "line": 24,
        "fullcodeline": "VLOG(2) << \"Variable: \" << var->name();"
    },
    {
        "line": 39,
        "fullcodeline": "all_nodes_.insert("
    },
    {
        "line": 51,
        "fullcodeline": "{\"ApplyProximalGradientDescent\", 4},"
    },
    {
        "line": 52,
        "fullcodeline": "{\"ApplyAdadelta\", 6},"
    },
    {
        "line": 53,
        "fullcodeline": "{\"ApplyAdagrad\", 3},"
    },
    {
        "line": 54,
        "fullcodeline": "{\"ApplyProximalAdagrad\", 5},"
    },
    {
        "line": 55,
        "fullcodeline": "{\"ApplyAdagradDA\", 3},"
    },
    {
        "line": 56,
        "fullcodeline": "{\"ApplyFtrl\", 3},"
    },
    {
        "line": 57,
        "fullcodeline": "{\"ApplyMomentum\", 3},"
    },
    {
        "line": 58,
        "fullcodeline": "{\"ApplyAdam\", 9},"
    },
    {
        "line": 59,
        "fullcodeline": "{\"ApplyRMSProp\", 7},"
    },
    {
        "line": 60,
        "fullcodeline": "{\"ApplyCenteredRMSProp\", 8}};"
    },
    {
        "line": 62,
        "fullcodeline": "auto apply_gradients_op = all_nodes_[apply_gradient_node_name]->op();"
    },
    {
        "line": 63,
        "fullcodeline": "auto apply_gradients_node = all_nodes_[apply_gradient_node_name];"
    },
    {
        "line": 65,
        "fullcodeline": "auto div_node = AddNodeDiv("
    },
    {
        "line": 69,
        "fullcodeline": "all_nodes_.insert(std::make_pair(div_node->name(), div_node));"
    },
    {
        "line": 70,
        "fullcodeline": "*apply_gradients_node->mutable_input(gradient_pos[apply_gradients_op]) ="
    },
    {
        "line": 89,
        "fullcodeline": "LOG(INFO) << \"Dequeue node: \" << dequeue_node->name();"
    },
    {
        "line": 90,
        "fullcodeline": "TF_RETURN_IF_ERROR(ComputeTransitiveFanin(graph_, {dequeue_node->name()},"
    },
    {
        "line": 97,
        "fullcodeline": "dont_replicate_nodes.insert(variable->name());"
    },
    {
        "line": 101,
        "fullcodeline": "dont_replicate_nodes.insert(NodeName(init));"
    },
    {
        "line": 8,
        "fullcodeline": "return Status(error::INVALID_ARGUMENT, \"No fetch nodes provided.\");"
    },
    {
        "line": 12,
        "fullcodeline": "return Status(error::INVALID_ARGUMENT, \"No variables provided.\");"
    },
    {
        "line": 40,
        "fullcodeline": "std::make_pair(graph_.node(i).name(), graph_.mutable_node(i)));"
    },
    {
        "line": 41,
        "fullcodeline": "if (apply_gradients_ops.find(graph_.node(i).op()) !="
    },
    {
        "line": 71,
        "fullcodeline": "div_node->name();"
    },
    {
        "line": 106,
        "fullcodeline": "if (input_node->name() != dequeue_node->name()) {"
    },
    {
        "line": 112,
        "fullcodeline": "if (dont_replicate_nodes.find(node->name()) == dont_replicate_nodes.end()) {"
    },
    {
        "line": 119,
        "fullcodeline": "if (replica_nodes_.find(node.first) == replica_nodes_.end()) {"
    },
    {
        "line": 42,
        "fullcodeline": "apply_gradients_ops.end()) {"
    },
    {
        "line": 43,
        "fullcodeline": "apply_gradients_nodes_.insert(graph_.node(i).name());"
    },
    {
        "line": 44,
        "fullcodeline": "VLOG(2) << \"Apply gradients node: \" << graph_.node(i).name();"
    },
    {
        "line": 67,
        "fullcodeline": "apply_gradients_node->input(gradient_pos[apply_gradients_op]),"
    },
    {
        "line": 68,
        "fullcodeline": "div_const_node->name());"
    },
    {
        "line": 82,
        "fullcodeline": "dequeue_node = train_node;"
    },
    {
        "line": 107,
        "fullcodeline": "dont_replicate_nodes.insert(input_node->name());"
    },
    {
        "line": 113,
        "fullcodeline": "replica_nodes_.insert(node->name());"
    },
    {
        "line": 120,
        "fullcodeline": "shared_nodes_.insert(node.first);"
    }
]