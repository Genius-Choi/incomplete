[
    {
        "line": 3,
        "fullcodeline": "Tensor x = context->input(0);"
    },
    {
        "line": 4,
        "fullcodeline": "const Tensor& scale = context->input(1);"
    },
    {
        "line": 5,
        "fullcodeline": "const Tensor& offset = context->input(2);"
    },
    {
        "line": 6,
        "fullcodeline": "const Tensor& estimated_mean = context->input(3);"
    },
    {
        "line": 7,
        "fullcodeline": "const Tensor& estimated_variance = context->input(4);"
    },
    {
        "line": 8,
        "fullcodeline": "const Tensor* side_input = has_side_input_ ? &context->input(5) : nullptr;"
    },
    {
        "line": 10,
        "fullcodeline": "OP_REQUIRES(context, x.dims() == 4 || x.dims() == 5,"
    },
    {
        "line": 13,
        "fullcodeline": "OP_REQUIRES(context, scale.dims() == 1,"
    },
    {
        "line": 16,
        "fullcodeline": "OP_REQUIRES(context, offset.dims() == 1,"
    },
    {
        "line": 19,
        "fullcodeline": "OP_REQUIRES(context, estimated_mean.dims() == 1,"
    },
    {
        "line": 22,
        "fullcodeline": "OP_REQUIRES("
    },
    {
        "line": 26,
        "fullcodeline": "bool use_reshape = (x.dims() == 5);"
    },
    {
        "line": 27,
        "fullcodeline": "auto x_shape = x.shape();"
    },
    {
        "line": 41,
        "fullcodeline": "const auto num_channels = GetTensorDim(x, tensor_format_, 'C');"
    },
    {
        "line": 42,
        "fullcodeline": "OP_REQUIRES("
    },
    {
        "line": 47,
        "fullcodeline": "OP_REQUIRES("
    },
    {
        "line": 84,
        "fullcodeline": "Tensor* y = nullptr;"
    },
    {
        "line": 85,
        "fullcodeline": "auto alloc_shape = use_reshape ? dest_shape : x_shape;"
    },
    {
        "line": 86,
        "fullcodeline": "OP_REQUIRES_OK(context, context->forward_input_or_allocate_output("
    },
    {
        "line": 89,
        "fullcodeline": "Tensor* batch_mean = nullptr;"
    },
    {
        "line": 90,
        "fullcodeline": "OP_REQUIRES_OK(context, context->forward_input_or_allocate_output("
    },
    {
        "line": 92,
        "fullcodeline": "Tensor* batch_var = nullptr;"
    },
    {
        "line": 93,
        "fullcodeline": "OP_REQUIRES_OK(context, context->forward_input_or_allocate_output("
    },
    {
        "line": 95,
        "fullcodeline": "Tensor* saved_mean = nullptr;"
    },
    {
        "line": 96,
        "fullcodeline": "OP_REQUIRES_OK(context,"
    },
    {
        "line": 98,
        "fullcodeline": "Tensor* saved_maybe_inv_var = nullptr;"
    },
    {
        "line": 99,
        "fullcodeline": "OP_REQUIRES_OK(context, context->allocate_output(4, scale.shape(),"
    },
    {
        "line": 11,
        "fullcodeline": "errors::InvalidArgument(\"input must be 4 or 5-dimensional\","
    },
    {
        "line": 14,
        "fullcodeline": "errors::InvalidArgument(\"scale must be 1-dimensional\","
    },
    {
        "line": 17,
        "fullcodeline": "errors::InvalidArgument(\"offset must be 1-dimensional\","
    },
    {
        "line": 20,
        "fullcodeline": "errors::InvalidArgument(\"estimated_mean must be 1-dimensional\","
    },
    {
        "line": 23,
        "fullcodeline": "context, estimated_variance.dims() == 1,"
    },
    {
        "line": 24,
        "fullcodeline": "errors::InvalidArgument(\"estimated_variance must be 1-dimensional\","
    },
    {
        "line": 43,
        "fullcodeline": "context, scale.NumElements() == num_channels,"
    },
    {
        "line": 44,
        "fullcodeline": "errors::InvalidArgument(\"scale must have the same number of elements \""
    },
    {
        "line": 48,
        "fullcodeline": "context, offset.NumElements() == num_channels,"
    },
    {
        "line": 49,
        "fullcodeline": "errors::InvalidArgument(\"offset must have the same number of elements \""
    },
    {
        "line": 52,
        "fullcodeline": "if (estimated_mean.NumElements() != 0) {"
    },
    {
        "line": 59,
        "fullcodeline": "if (estimated_variance.NumElements() != 0) {"
    },
    {
        "line": 75,
        "fullcodeline": "if (activation_mode_ != FbnActivationMode::kIdentity) {"
    },
    {
        "line": 97,
        "fullcodeline": "context->allocate_output(3, scale.shape(), &saved_mean));"
    },
    {
        "line": 12,
        "fullcodeline": "x.shape().DebugString()));"
    },
    {
        "line": 15,
        "fullcodeline": "scale.shape().DebugString()));"
    },
    {
        "line": 18,
        "fullcodeline": "offset.shape().DebugString()));"
    },
    {
        "line": 21,
        "fullcodeline": "estimated_mean.shape().DebugString()));"
    },
    {
        "line": 25,
        "fullcodeline": "estimated_variance.shape().DebugString()));"
    },
    {
        "line": 30,
        "fullcodeline": "const int64_t in_batch = GetTensorDim(x, tensor_format_, 'N');"
    },
    {
        "line": 31,
        "fullcodeline": "int64_t in_planes = GetTensorDim(x, tensor_format_, '0');"
    },
    {
        "line": 32,
        "fullcodeline": "int64_t in_rows = GetTensorDim(x, tensor_format_, '1');"
    },
    {
        "line": 33,
        "fullcodeline": "int64_t in_cols = GetTensorDim(x, tensor_format_, '2');"
    },
    {
        "line": 34,
        "fullcodeline": "const int64_t in_depth = GetTensorDim(x, tensor_format_, 'C');"
    },
    {
        "line": 35,
        "fullcodeline": "dest_shape = ShapeFromFormat(tensor_format_, in_batch,"
    },
    {
        "line": 37,
        "fullcodeline": "OP_REQUIRES(context, x.CopyFrom(x, dest_shape),"
    },
    {
        "line": 46,
        "fullcodeline": "scale.NumElements(), \" and \", num_channels));"
    },
    {
        "line": 51,
        "fullcodeline": "offset.NumElements(), \" and \", num_channels));"
    },
    {
        "line": 53,
        "fullcodeline": "OP_REQUIRES(context, estimated_mean.NumElements() == num_channels,"
    },
    {
        "line": 60,
        "fullcodeline": "OP_REQUIRES(context, estimated_variance.NumElements() == num_channels,"
    },
    {
        "line": 68,
        "fullcodeline": "OP_REQUIRES(context, side_input->shape() == x.shape(),"
    },
    {
        "line": 78,
        "fullcodeline": "OP_REQUIRES("
    },
    {
        "line": 87,
        "fullcodeline": "{0}, 0, alloc_shape, &y));"
    },
    {
        "line": 91,
        "fullcodeline": "{3}, 1, scale.shape(), &batch_mean));"
    },
    {
        "line": 94,
        "fullcodeline": "{4}, 2, scale.shape(), &batch_var));"
    },
    {
        "line": 103,
        "fullcodeline": "functor::FusedBatchNorm<Device, T, U, true>()("
    },
    {
        "line": 116,
        "fullcodeline": "OP_REQUIRES(context, y->CopyFrom(*y, x_shape),"
    },
    {
        "line": 38,
        "fullcodeline": "errors::InvalidArgument(\"Error during tensor copy.\"));"
    },
    {
        "line": 54,
        "fullcodeline": "errors::InvalidArgument("
    },
    {
        "line": 61,
        "fullcodeline": "errors::InvalidArgument("
    },
    {
        "line": 69,
        "fullcodeline": "errors::InvalidArgument("
    },
    {
        "line": 79,
        "fullcodeline": "context, !is_training_ || num_channels % 4 == 0,"
    },
    {
        "line": 80,
        "fullcodeline": "errors::InvalidArgument(\"FusedBatchNorm with activation requires \""
    },
    {
        "line": 109,
        "fullcodeline": "functor::FusedBatchNorm<Device, T, U, false>()("
    },
    {
        "line": 117,
        "fullcodeline": "errors::InvalidArgument(\"Error during tensor copy.\"));"
    },
    {
        "line": 36,
        "fullcodeline": "{{in_planes, in_rows * in_cols}}, in_depth);"
    },
    {
        "line": 57,
        "fullcodeline": "estimated_mean.NumElements(), \" and \", num_channels));"
    },
    {
        "line": 64,
        "fullcodeline": "estimated_variance.NumElements(), \" and \", num_channels));"
    },
    {
        "line": 71,
        "fullcodeline": "side_input->shape().DebugString(),"
    },
    {
        "line": 72,
        "fullcodeline": "\" != \", x.shape().DebugString()));"
    }
]