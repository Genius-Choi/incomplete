void FusedConvParallelFor(
    OpKernelContext* context, int64_t begin, int64_t end,
    const std::function<void(int64_t, int64_t)>& task_function) {
// On iOS, the thread management imposes a very big performance penalty, so
// just call the function directly with no multithreading.
#if defined(__APPLE__) && defined(IS_MOBILE_PLATFORM)
  task_function(begin, end);
#else
  auto& worker_threads = *(context->device()->tensorflow_cpu_worker_threads());
  thread::ThreadPool* thread_pool = worker_threads.workers;
  const int64_t total_elements = end - begin;
  // This is a bit of an arbitrary number, but was found to work well for
  // typical models we've been profiling on various devices.
  const int64_t element_cost = 10000000;
  thread_pool->ParallelFor(
      total_elements, element_cost,
      [begin, task_function](int64_t begin_offset, int64_t end_offset) {
        const int64_t task_begin = begin + begin_offset;
        const int64_t task_end = begin + end_offset;
        task_function(task_begin, task_end);
      });
#endif
}
