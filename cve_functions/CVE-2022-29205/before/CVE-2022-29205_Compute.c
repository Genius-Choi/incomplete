  void Compute(OpKernelContext* context) override {
    TensorShape processing_shape, final_shape;
    bool is_identity = true;
    bool slice_dim0 = true;
    bool is_simple_slice = true;
    gtl::InlinedVector<int64_t, 4> begin;
    gtl::InlinedVector<int64_t, 4> end;
    gtl::InlinedVector<int64_t, 4> strides;

    Tensor* old_lhs = nullptr;
    Tensor tmp;
    if (isTensor) {
      const Tensor& input = context->input(0);

      int forwarded_input;
      OP_REQUIRES_OK(context,
                     context->forward_input_or_allocate_output(
                         {0}, 0, input.shape(), &old_lhs, &forwarded_input));
      if (forwarded_input < 0) {
        OP_REQUIRES_OK(context,
                       tensorflow::functor::DoCopy(
                           context->eigen_device<Device>(), input, old_lhs));
      }
    } else {
      if (context->input_dtype(0) == DT_RESOURCE) {
        core::RefCountPtr<Var> v;
        OP_REQUIRES_OK(
            context, LookupResource(context, HandleFromInput(context, 0), &v));
        OP_REQUIRES_OK(context,
                       EnsureSparseVariableAccess<Device, T>(context, v.get()));
        mutex_lock ml(*v->mu());
        old_lhs = v->tensor();
        OP_REQUIRES(context, old_lhs->dtype() == DataTypeToEnum<T>::value,
                    errors::InvalidArgument(
                        "l-value dtype ", DataTypeString(old_lhs->dtype()),
                        " does not match r-value dtype ",
                        DataTypeString(DataTypeToEnum<T>::value)));
      } else {
        context->forward_ref_input_to_ref_output(0, 0);
        tmp = context->mutable_input(0, true);
        old_lhs = &tmp;
      }
    }

    OP_REQUIRES_OK(
        context, ValidateStridedSliceOp(
                     &context->input(1), &context->input(2), context->input(3),
                     old_lhs->shape(), begin_mask, end_mask, ellipsis_mask,
                     new_axis_mask, shrink_axis_mask, &processing_shape,
                     &final_shape, &is_identity, &is_simple_slice, &slice_dim0,
                     &begin, &end, &strides));

    if (processing_shape.num_elements()) {
      const Tensor& input = context->input(4);
      TensorShape input_shape = input.shape();
      TensorShape original_shape = old_lhs->shape();
      // TODO(aselle): This check is too strong, we only should need
      // input_shape to be broadcastable to final_shape
      OP_REQUIRES(
          context, final_shape == input_shape,
          errors::Unimplemented(
              "sliced l-value shape ", final_shape.DebugString(),
              " does not match r-value shape ", input_shape.DebugString(),
              ". Automatic broadcasting not ", "yet implemented."));
      const int processing_dims = processing_shape.dims();

      // 0-dimensional case implies the left and right are exactly the same
      // scalar shape

// Handle general dimensions
#define HANDLE_DIM(NDIM)                                                       \
  if (processing_dims == NDIM) {                                               \
    HandleStridedSliceAssignCase<Device, T, NDIM>()(context, begin, end,       \
                                                    strides, processing_shape, \
                                                    is_simple_slice, old_lhs); \
    return;                                                                    \
  }
      HANDLE_DIM(0);
      HANDLE_DIM(1);
      HANDLE_DIM(2);
      HANDLE_DIM(3);
      HANDLE_DIM(4);
      HANDLE_DIM(5);
      HANDLE_DIM(6);
      HANDLE_DIM(7);
      HANDLE_DIM(8);
#undef HANDLE_DIM

      OP_REQUIRES(context, false,
                  errors::Unimplemented("Unhandled input dimensions ",
                                        processing_dims));
    }
  }
