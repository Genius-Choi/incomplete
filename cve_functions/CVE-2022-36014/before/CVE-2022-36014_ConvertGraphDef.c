StatusOr<OwningOpRef<ModuleOp>> GraphDefImporter::ConvertGraphDef(
    const GraphDef &graph) {
  // Create the module.
  OwningOpRef<ModuleOp> module = ModuleOp::create(unknown_loc_);

  // Create the graph op.
  auto builder = OpBuilder::atBlockBegin(module->getBody());
  auto graph_op = builder.create<GraphOp>(
      module->getLoc(), ConvertVersionAttr(ctx_, graph.versions()));
  graph_op.nodes().push_back(new Block);

  // Populate the function op defs.
  function_op_defs_.reserve(graph.library().function_size());
  for (const FunctionDef &function : graph.library().function()) {
    function_op_defs_.emplace(function.signature().name(),
                              &function.signature());
  }

  // Build a map from function name to gradient function name.
  absl::flat_hash_map<StringPiece, StringPiece> gradient_map;
  gradient_map.reserve(graph.library().gradient_size());
  for (const tensorflow::GradientDef &gradient : graph.library().gradient())
    gradient_map.emplace(gradient.function_name(), gradient.gradient_func());

  // Convert the graph.
  ConversionState s(&graph_op.nodes().front(), placeholder_state_);
  TF_RETURN_IF_ERROR(
      ConvertNodes(builder, s, graph.node(), &graph_op.nodes().front()));

  // A function to convert a generic or non-generic function.
  const auto convert_func = [this, &gradient_map](GraphFuncOp func_op,
                                                  const FunctionDef &function) {
    if (IsGenericFunction(function)) {
      // Generic functions aren't on the hot path so just call the old
      // importer.
      OpBuilder builder(ctx_);
      TF_RETURN_WITH_CONTEXT_IF_ERROR(
          ConvertGenericFunction(func_op, function, builder),
          "While importing generic function: ", function.signature().name());
    } else {
      TF_RETURN_WITH_CONTEXT_IF_ERROR(
          ConvertFunctionDef(func_op, gradient_map, function),
          "While importing function: ", function.signature().name());
    }
    return Status::OK();
  };

  // TODO(jeffniu): Don't import functions in parallel if there are too few (how
  // few?) or if the functions are too small (how small?).
  if (ctx_->isMultithreadingEnabled()) {
    ctx_->enterMultiThreadedExecution();
    auto exit =
        llvm::make_scope_exit([this] { ctx_->exitMultiThreadedExecution(); });

    // Prepare the arguments to parallel for each.
    struct Argument {
      GraphFuncOp func;
      const FunctionDef &def;
      Status status;
    };
    std::vector<Argument> args;
    args.reserve(graph.library().function_size());
    for (const FunctionDef &function : graph.library().function()) {
      args.push_back(
          Argument{builder.create<GraphFuncOp>(unknown_loc_), function});
    }
    const auto process_func = [&convert_func](Argument &arg) {
      arg.status = convert_func(arg.func, arg.def);
      return success(arg.status.ok());
    };

    // Execute the imports in parallel.
    if (failed(failableParallelForEach(ctx_, args, process_func))) {
      Status result;
      for (const Argument &arg : args) {
        result.Update(arg.status);
      }
      return result;
    }
  } else {
    // Convert the functions.
    for (const FunctionDef &function : graph.library().function()) {
      auto func_op = builder.create<GraphFuncOp>(unknown_loc_);
      TF_RETURN_IF_ERROR(convert_func(func_op, function));
    }
  }

  return module;
}
