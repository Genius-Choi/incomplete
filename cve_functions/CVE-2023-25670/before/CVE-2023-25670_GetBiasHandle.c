  Tbias* GetBiasHandle(
      OpKernelContext* context,
      std::shared_ptr<dnnl::inner_product_forward::primitive_desc>&
          mkldnn_matmul_fwd_pd,
      const Tensor& bias_tensor, const Tensor& weight_tensor,
      std::shared_ptr<stream> reorder_stream) {
    // If the bias is qint32, it means the bias is already converted offline.
    // and it can be added to matmul output directly.
    if (std::is_same<Tbias, qint32>::value) {
      return static_cast<Tbias*>(
          const_cast<Tbias*>(bias_tensor.flat<Tbias>().data()));
    } else {
      // If the bias is fp32, then need to calculate the bias
      const float min_input = context->input(3).flat<float>()(0);
      const float max_input = context->input(4).flat<float>()(0);
      const float min_weight = context->input(5).flat<float>()(0);
      const float max_weight = context->input(6).flat<float>()(0);

      std::vector<dnnl::primitive> net;
      float out_scale;
      // If the bias is float and input quantize is MIN_FIRST, bias has to be
      // compensated with B's32 = Q'a * Qw * Bf32 + Q'a * Qw * Min(Af32) * 1 *
      // Wf32.
      if (mode_ == QUANTIZE_MODE_MIN_FIRST) {
        int k = weight_tensor.dim_size(0);
        int n = weight_tensor.dim_size(1);
        float* comp_bias = GetCompBiasBuffer(n);

        qint8* wt_buf = static_cast<qint8*>(
            const_cast<qint8*>(weight_tensor.flat<qint8>().data()));

        const float* bias_buf = static_cast<float*>(
            const_cast<float*>(bias_tensor.flat<float>().data()));

        float qa_amin = 255 * min_input / (max_input - min_input);

        out_scale = (255.0 * 127.0) /
                    ((max_input - min_input) *
                     std::max(std::abs(max_weight), std::abs(min_weight)));

#ifndef ENABLE_ONEDNN_OPENMP
        auto parallel_func = [&](int64 start, int64 end) {
          for (int64 j = start; j < end; j++) {
            int x = 0;
            for (int64 i = 0; i < k; ++i) {
              x += wt_buf[i * n + j];
            }
            comp_bias[j] =
                ((bias_buf[j] * out_scale) + static_cast<float>(x * qa_amin));
          }
        };

        const float kArithCost = 2.5f;
        const float kMovCost = 1.0f;
        float shard_cost = 4 * kArithCost + kMovCost;
        const DeviceBase::CpuWorkerThreads& worker_threads =
            *(context->device()->tensorflow_cpu_worker_threads());
        Shard(worker_threads.num_threads, worker_threads.workers, n, shard_cost,
              parallel_func);
#else
#pragma omp parallel for schedule(static)
        for (int j = 0; j < n; ++j) {
          int x = 0;
          for (int i = 0; i < k; ++i) {
            x += wt_buf[i * n + j];
          }
          comp_bias[j] =
              ((bias_buf[j] * out_scale) + static_cast<float>(x * qa_amin));
        }
#endif  // !ENABLE_ONEDNN_OPENMP
        return reinterpret_cast<Tbias*>(comp_bias_);

      } else if (mode_ == QUANTIZE_MODE_SCALED) {
        // If the bias is float and input quantize is SCALE, bias has to be
        // compensated with Bs32 = Qa * Qw * Bf32.
        out_scale = 255.0 * 127.0 / max_input *
                    std::max(std::abs(max_weight), std::abs(min_weight));

        std::vector<float> scales;
        scales.push_back(out_scale);
        dnnl::primitive_attr bias_attr;
        bias_attr.set_output_scales(0, scales);

        void* bias_buf = static_cast<void*>(
            const_cast<Tbias*>(bias_tensor.flat<Tbias>().data()));
        input_bias_ = new memory(mkldnn_matmul_fwd_pd->bias_desc(),
                                 this->cpu_engine_, bias_buf);
        scaled_bias_ =
            new memory(mkldnn_matmul_fwd_pd->bias_desc(), this->cpu_engine_);

        auto reorder_desc = dnnl::reorder::primitive_desc(
            *input_bias_, *scaled_bias_, bias_attr);
        net.push_back(dnnl::reorder(reorder_desc));
        std::unordered_map<int, memory> reorder_net_args = {
            {DNNL_ARG_FROM, *input_bias_}, {DNNL_ARG_TO, *scaled_bias_}};
        net.at(0).execute(*reorder_stream, reorder_net_args);

        return reinterpret_cast<Tbias*>(scaled_bias_->get_data_handle());
      } else {
        context->CtxFailure(
            errors::InvalidArgument("Quantization mode must be"
                                    "either MIN_FIRST or SCALED."));
        return nullptr;
      }
    }
  }
