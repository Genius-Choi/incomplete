static tsl::StatusOr<cublasMath_t> GetMathTypeForGemmEx(
    Stream *stream, blas::AlgorithmType algorithm, blas::DataType type_a,
    blas::DataType type_b, blas::ComputePrecision precision) {
  if (type_a != type_b) {
    return tsl::errors::Internal("Types of inputs mismatch");
  }

  // GPUs < sm_50 don't support cublasGemmEx.
  CudaComputeCapability cc = stream->GetCudaComputeCapability();
  if (cc.major < 5) {
    return tsl::errors::Internal("sm_", cc.major,
                                 " does not support explicit gemm algorithms.");
  }

  bool algo_uses_tensor_ops = UsesTensorOps(algorithm);
  cublasMath_t math_type = CUBLAS_DEFAULT_MATH;
  if (algo_uses_tensor_ops) {
    if (cc.major < 7) {
      return tsl::errors::Internal(
          "Algorithm ", algorithm,
          " uses tensor ops, but tensor ops are not available in sm", cc.major,
          "X devices.");
    } else if (type_a == blas::DataType::kFloat) {
#if CUDA_VERSION < 11000
      return tsl::errors::Internal(
          "Algorithm ", algorithm,
          " uses tensor ops, but tensor ops are not available for fp32");
#else
      if (cc.major < 8) {
        return tsl::errors::Internal(
            "Algorithm ", algorithm,
            " uses tensor ops, but tensor ops are not available in sm",
            cc.major, "X devices for float input types.");
      } else if (!tsl::tensor_float_32_execution_enabled()) {
        return tsl::errors::Internal(
            "Algorithm ", algorithm,
            " uses tensor ops, but tensor ops are disabled for fp32 inputs");
      }
      math_type = CUBLAS_TF32_TENSOR_OP_MATH;
#endif
    } else if (type_a == blas::DataType::kHalf) {
#if CUDA_VERSION < 11000
      math_type = CUBLAS_TENSOR_OP_MATH;
#endif
    } else {
      return tsl::errors::Internal(
          "Algorithm ", algorithm,
          " uses tensor ops which are not supported for input");
    }
  }
  if (precision > blas::kDefaultComputePrecision) {
    math_type = CUBLAS_DEFAULT_MATH;
  }

  // Return false if we might be hitting a cuBLAS bug that produces the wrong
  // result. See nvbugs/2156201, b/79126339.
#if CUDA_VERSION >= 9000 && CUDA_VERSION < 9020
  if ((algorithm == CUBLAS_GEMM_DEFAULT || algorithm >= CUBLAS_GEMM_ALGO13) &&
      std::max({m, n, k}) >= 2097153 && cc_major < 7) {
    return tsl::errors::Internal(
        "DoBlasGemmWithAlgorithm returning false to work around cudnn "
        "<9.2 bug with m, n, or k >= 2097153.  See b/79126339.");
  }
#endif
  return math_type;
}
