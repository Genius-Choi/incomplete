Status TPUPartitionedCallOp::ReplaceAndPartitionXLAShardingVariable(
    Graph* graph, OpKernelContext* ctx, int device_ordinal,
    ResourceHandle& handle, Node* variable, const TPUMetadata& tpu_metadata) {
  if (device_ordinal >= tpu_metadata.topology.num_tpu_devices_per_task()) {
    return errors::InvalidArgument(
        "There are ", tpu_metadata.topology.num_tpu_devices_per_task(),
        " TPU devices, however selected device_ordinal: ", device_ordinal,
        " exceeds the range");
  }

  TF_ASSIGN_OR_RETURN(
      auto sharding,
      GetShardingFromNodeDef(variable->def(), /*add_metadata=*/false));
  xla::OpSharding xla_sharding;
  bool is_var_sharded = false;
  if (sharding.has_value() &&
      sharding.value().type() == xla::OpSharding::OTHER) {
    xla_sharding = sharding.value();
    for (int dim = 0; dim < GetDimsFromXLAShardingTiled(xla_sharding); dim++) {
      is_var_sharded |= xla_sharding.tile_assignment_dimensions(dim) > 1;
    }
  } else {
    xla_sharding.set_type(xla::OpSharding::REPLICATED);
    is_var_sharded = false;
  }

  core::RefCountPtr<Var> var;
  TF_RETURN_IF_ERROR(LookupResource(ctx, handle, &var));

  VLOG(3) << "Replace and partition variable " << variable->name()
          << " is_var_sharded: " << is_var_sharded
          << " shape: " << var->tensor()->shape().DebugString()
          << " with xla_sharding: " << xla_sharding.DebugString();

  int split_dim = -1;
  int split_size = 0;

  if (is_var_sharded) {
    for (int dim = 0; dim < GetDimsFromXLAShardingTiled(xla_sharding); dim++) {
      if (xla_sharding.tile_assignment_dimensions(dim) > 1) {
        if (split_dim != -1) {
          return errors::InvalidArgument(
              "Currently we only support inference with one split dimension, "
              "however got sharding: ",
              xla_sharding.DebugString());
        }
        split_dim = dim;
        split_size = xla_sharding.tile_assignment_dimensions(dim);
      }
    }
    if (split_dim == -1 || split_dim >= var->tensor()->dims()) {
      return errors::InvalidArgument(
          "sharding split_dim ", split_dim, " for variable: ", variable->name(),
          " is -1 or large than the number of dimensions ",
          var->tensor()->dims());
    }
  }

  const auto& topology = tpu_metadata.topology;
  int num_cores_per_replica = tpu_metadata.num_cores_per_replica;
  xla::Array4D<int> mapping(topology.mesh_shape(0), topology.mesh_shape(1),
                            topology.mesh_shape(2), topology.mesh_shape(3), -1);
  int pos = 0;
  // The topology should only have one task.
  for (int device = 0; device < topology.num_tpu_devices_per_task(); device++) {
    int32_t x = topology.device_coordinates(pos++);
    int32_t y = topology.device_coordinates(pos++);
    int32_t z = topology.device_coordinates(pos++);
    int32_t core = topology.device_coordinates(pos++);
    mapping(x, y, z, core) = device;
  }

  const string cname = ctx->resource_manager()->default_container();
  std::vector<Node*> per_core_vars;
  std::vector<string> tpu_devices;
  for (int i = 0; i < num_cores_per_replica; i++) {
    int offset = i * 4;
    int device_index = mapping(tpu_metadata.device_assignment[offset],
                               tpu_metadata.device_assignment[offset + 1],
                               tpu_metadata.device_assignment[offset + 2],
                               tpu_metadata.device_assignment[offset + 3]);

    NodeDef ndef;
    uint64 fp = Fingerprint64(
        strings::StrCat(handle.container(), handle.name(), "_", device_index));
    ndef.set_name(strings::StrCat(handle.name(), fp));
    ndef.set_op(kVarHandleOp);
    string tpu_device = strings::StrCat(kTPUDeviceNamePrefix, device_index);
    ndef.set_device(tpu_device);
    tpu_devices.push_back(tpu_device);

    // Replace each _Arg node of type DT_RESOURCE that goes into a TPU node
    // by a VarHandleOp on TPU with shared_name "v_tpu_x" where "v" is the
    // shared_name of the variable on CPU and "x" is the rewritten device
    // ordinal.
    const string sname = strings::StrCat(handle.name(), "_tpu_", device_index);
    AddNodeAttr("shared_name", sname, &ndef);
    AddNodeAttr("container", cname, &ndef);
    AddNodeAttr("dtype", var->tensor()->dtype(), &ndef);

    TensorShapeProto proto;
    var->tensor()->shape().AsProto(&proto);

    if (is_var_sharded) {
      int dim_size = proto.dim(split_dim).size();
      if (dim_size % split_size != 0) {
        return errors::InvalidArgument("dimension size ", dim_size,
                                       " cannot be divisible by split size ",
                                       split_size);
      }
      proto.mutable_dim(split_dim)->set_size(dim_size / split_size);
    }
    AddNodeAttr("shape", proto, &ndef);

    TF_ASSIGN_OR_RETURN(Node * new_node, graph->AddNode(ndef));
    per_core_vars.push_back(new_node);
  }

  // Insert TPUPartitionedInput op.
  NodeDefBuilder builder(absl::StrCat(handle.name(), "/tpu_partitioned_input"),
                         "TPUPartitionedInput");
  builder.Attr("N", num_cores_per_replica);
  builder.Attr("T", DT_RESOURCE);
  builder.Attr("partition_dim", split_dim);
  builder.Attr("_XlaSharding", xla_sharding.SerializeAsString());
  std::vector<NodeDefBuilder::NodeOut> inputs;
  inputs.reserve(num_cores_per_replica);
  for (int i = 0; i < num_cores_per_replica; i++) {
    inputs.push_back({per_core_vars[i]->name(), 0, DT_RESOURCE});
  }
  builder.Input(inputs);
  NodeDef node_def;
  TF_RETURN_IF_ERROR(builder.Finalize(&node_def));
  TF_ASSIGN_OR_RETURN(Node * tpu_partitioned_input_node,
                      graph->AddNode(node_def));

  for (int i = 0; i < num_cores_per_replica; i++) {
    graph->AddEdge(per_core_vars[i], 0, tpu_partitioned_input_node, i);
  }

  // Insert TPUReplicatedInput op.
  NodeDefBuilder replicated_builder(
      absl::StrCat(handle.name(), "/tpu_replicated_input"),
      "TPUReplicatedInput");
  replicated_builder.Attr("N", 1);
  replicated_builder.Attr("T", DT_RESOURCE);
  replicated_builder.Attr("is_mirrored_variable", true);
  std::vector<NodeDefBuilder::NodeOut> replicated_inputs;
  replicated_inputs.push_back(
      {tpu_partitioned_input_node->name(), 0, DT_RESOURCE});
  replicated_builder.Input(replicated_inputs);
  NodeDef replicated_node_def;
  TF_RETURN_IF_ERROR(replicated_builder.Finalize(&replicated_node_def));
  Status replicated_s;
  Node* tpu_replicated_input_node =
      graph->AddNode(replicated_node_def, &replicated_s);
  if (!replicated_s.ok()) {
    return replicated_s;
  }
  graph->AddEdge(tpu_partitioned_input_node, 0, tpu_replicated_input_node, 0);

  // Connect the TPUReplicatedInput node to the previous output nodes of the
  // variable, and remove the variable node.
  std::vector<Node*> dst_nodes;
  std::vector<int> src_indices;
  std::vector<int> dst_indices;
  for (const Edge* edge : variable->out_edges()) {
    dst_nodes.push_back(edge->dst());
    src_indices.push_back(edge->src_output());
    dst_indices.push_back(edge->dst_input());
  }
  for (int i = 0; i < dst_nodes.size(); i++) {
    graph->AddEdge(tpu_replicated_input_node, src_indices[i], dst_nodes[i],
                   dst_indices[i]);
  }

  graph->RemoveNode(variable);

  std::vector<NodeDef> ndefs;
  Status status;
  for (int i = 0; i < num_cores_per_replica; i++) {
    Device* d;
    TF_RETURN_IF_ERROR(
        library_runtime_->device_mgr()->LookupDevice(tpu_devices[i], &d));
    string sname;
    const NodeDef& ndef = per_core_vars[i]->def();
    TF_RETURN_IF_ERROR(GetNodeAttr(ndef, "shared_name", &sname));
    ndefs.push_back(ndef);
    Var* tpu_var;
    status = d->resource_manager()->Lookup(cname, sname, &tpu_var);
  }

  if (!status.ok()) {
    TF_RETURN_IF_ERROR(
        InitializeShardedVarOnTPU(ctx, var, ndefs, split_dim, tpu_devices));
    if (VLOG_IS_ON(4)) {
      for (int i = 0; i < num_cores_per_replica; i++) {
        string sname;
        TF_RETURN_IF_ERROR(GetNodeAttr(ndefs[i], "shared_name", &sname));
        LOG(INFO) << "Initialized sharded variable on TPU: " << sname
                  << " device: " << tpu_devices[i];
      }
    }
  }

  return OkStatus();
}
