void __io_uring_task_cancel(void)
{
	struct io_uring_task *tctx = current->io_uring;
	DEFINE_WAIT(wait);
	long completions;

	/* make sure overflow events are dropped */
	tctx->in_idle = true;

	while (!io_uring_task_idle(tctx)) {
		/* read completions before cancelations */
		completions = atomic_long_read(&tctx->req_complete);
		__io_uring_files_cancel(NULL);

		prepare_to_wait(&tctx->wait, &wait, TASK_UNINTERRUPTIBLE);

		/*
		 * If we've seen completions, retry. This avoids a race where
		 * a completion comes in before we did prepare_to_wait().
		 */
		if (completions != atomic_long_read(&tctx->req_complete))
			continue;
		if (io_uring_task_idle(tctx))
			break;
		schedule();
	}

	finish_wait(&tctx->wait, &wait);
	tctx->in_idle = false;
}
