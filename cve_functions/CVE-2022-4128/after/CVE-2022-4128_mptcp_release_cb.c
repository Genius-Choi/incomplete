static void mptcp_release_cb(struct sock *sk)
	__must_hold(&sk->sk_lock.slock)
{
	struct mptcp_sock *msk = mptcp_sk(sk);

	for (;;) {
		unsigned long flags = (msk->cb_flags & MPTCP_FLAGS_PROCESS_CTX_NEED) |
				      msk->push_pending;
		if (!flags)
			break;

		/* the following actions acquire the subflow socket lock
		 *
		 * 1) can't be invoked in atomic scope
		 * 2) must avoid ABBA deadlock with msk socket spinlock: the RX
		 *    datapath acquires the msk socket spinlock while helding
		 *    the subflow socket lock
		 */
		msk->push_pending = 0;
		msk->cb_flags &= ~flags;
		spin_unlock_bh(&sk->sk_lock.slock);
		if (flags & BIT(MPTCP_FLUSH_JOIN_LIST))
			__mptcp_flush_join_list(sk);
		if (flags & BIT(MPTCP_PUSH_PENDING))
			__mptcp_push_pending(sk, 0);
		if (flags & BIT(MPTCP_RETRANSMIT))
			__mptcp_retrans(sk);

		cond_resched();
		spin_lock_bh(&sk->sk_lock.slock);
	}

	if (__test_and_clear_bit(MPTCP_CLEAN_UNA, &msk->cb_flags))
		__mptcp_clean_una_wakeup(sk);
	if (unlikely(&msk->cb_flags)) {
		/* be sure to set the current sk state before tacking actions
		 * depending on sk_state, that is processing MPTCP_ERROR_REPORT
		 */
		if (__test_and_clear_bit(MPTCP_CONNECTED, &msk->cb_flags))
			__mptcp_set_connected(sk);
		if (__test_and_clear_bit(MPTCP_ERROR_REPORT, &msk->cb_flags))
			__mptcp_error_report(sk);
		if (__test_and_clear_bit(MPTCP_RESET_SCHEDULER, &msk->cb_flags))
			msk->last_snd = NULL;
	}

	__mptcp_update_rmem(sk);
}
