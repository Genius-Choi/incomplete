void gf_filter_pid_send_event_downstream(GF_FSTask *task)
{
	u32 i, count, nb_playing=0, nb_paused=0;
	Bool canceled = GF_FALSE;
	Bool forced_cancel = GF_FALSE;
	GF_FilterEvent *evt = task->udta;
	GF_Filter *f = task->filter;
	GF_List *dispatched_filters = NULL;
	GF_FilterPidInst *for_pidi = (GF_FilterPidInst *)task->pid;

	if (for_pidi && (for_pidi->pid == task->pid)) {
		for_pidi = NULL;
	}

	//if stream reset task is posted, wait for it before processing this event
	if (f->stream_reset_pending) {
		TASK_REQUEUE(task)
		return;
	}
	//if some pids are still detached, wait for the connection before processing this event
	if (f->detached_pid_inst) {
		TASK_REQUEUE(task)
		task->can_swap = 1;
		return;
	}

	if (evt->base.on_pid) {
		assert(evt->base.on_pid->filter->num_events_queued);
		safe_int_dec(&evt->base.on_pid->filter->num_events_queued);
	}
	if (f->finalized) {
		free_evt(evt);
		return;
	}

	if (for_pidi) {
		//update pid instance status
		switch (evt->base.type) {
		case GF_FEVT_PLAY:
		case GF_FEVT_SOURCE_SEEK:
			for_pidi->is_playing = GF_TRUE;
			for_pidi->play_queued = 0;
			break;
		case GF_FEVT_STOP:
			for_pidi->is_playing = GF_FALSE;
			for_pidi->stop_queued = 0;
			break;
		case GF_FEVT_PAUSE:
			for_pidi->is_paused = GF_TRUE;
			break;
		case GF_FEVT_RESUME:
			for_pidi->is_paused = GF_FALSE;
			break;
		default:
			break;
		}
	}
	if (evt->base.on_pid) {
		GF_FilterPid *pid = (GF_FilterPid *) evt->base.on_pid->pid;
		//we have destination for this pid but this is an event not targeting a dedicated pid instance, this
		//means this was a connect fail, do not process event as other parts of the graphs are using this filter
		if (pid->num_destinations && !for_pidi
			&& ((evt->base.type==GF_FEVT_PLAY) || (evt->base.type==GF_FEVT_STOP) || (evt->base.type==GF_FEVT_CONNECT_FAIL))
		) {
			//we incremented discard counter in gf_filter_pid_send_event_internal for stop, decrement
			//this typically happen when pid has 2 destinations, one OK and the other one failed to configure
			if (evt->base.type==GF_FEVT_STOP) {
				for (i=0; i<pid->num_destinations; i++) {
					for_pidi = gf_list_get(pid->destinations, i);
					if (for_pidi->discard_packets)
						safe_int_dec(&for_pidi->discard_packets);
				}
			}
			free_evt(evt);
			return;
		}
		//update number of playing/paused pids
		for (i=0; i<pid->num_destinations; i++) {
			GF_FilterPidInst *pidi = gf_list_get(pid->destinations, i);
			if (pidi->is_playing) nb_playing++;
			if (pidi->is_paused) nb_paused++;
		}
	}

	if (evt->base.type == GF_FEVT_BUFFER_REQ) {
		if (!evt->base.on_pid) {
			free_evt(evt);
			return;
		}
		//, set buffering at this level if:
		if (
			//- pid is a decoder input
			evt->base.on_pid->nb_decoder_inputs
			//- buffer req event explicitly requested for this pid
			|| evt->buffer_req.pid_only
			//- pid is a raw media source
			|| filter_pid_is_raw_source(evt->base.on_pid)
		) {
			evt->base.on_pid->max_buffer_time = evt->base.on_pid->user_max_buffer_time = evt->buffer_req.max_buffer_us;
			evt->base.on_pid->user_max_playout_time = evt->buffer_req.max_playout_us;
			evt->base.on_pid->user_min_playout_time = evt->buffer_req.min_playout_us;
			evt->base.on_pid->max_buffer_unit = 0;
			evt->base.on_pid->user_buffer_forced = evt->buffer_req.pid_only;
			//update blocking state
			if (evt->base.on_pid->would_block)
				gf_filter_pid_check_unblock(evt->base.on_pid);
			else
				gf_filter_pid_would_block(evt->base.on_pid);
			canceled = GF_TRUE;
		} else {
			evt->base.on_pid->user_buffer_forced = GF_FALSE;
		}
	} else if (evt->base.on_pid && (evt->base.type == GF_FEVT_PLAY)
		&& (evt->base.on_pid->pid->is_playing || (((GF_FilterPid *) evt->base.on_pid->pid)->not_connected==2))
		) {
		if (evt->base.on_pid->pid->is_playing) {
			GF_LOG(GF_LOG_INFO, GF_LOG_FILTER, ("Filter %s PID %s event %s but PID is already playing, discarding\n", f->name, evt->base.on_pid->name, gf_filter_event_name(evt->base.type)));
		}
		free_evt(evt);
		return;
	} else if (evt->base.on_pid && (evt->base.type == GF_FEVT_STOP)
		&& (
			//stop request on pid already stop
			!evt->base.on_pid->pid->is_playing
			//fan out but some instances are still playing
			|| nb_playing
		)
	) {
		GF_FilterPid *pid = (GF_FilterPid *) evt->base.on_pid->pid;

		if (!evt->base.on_pid->pid->is_playing) {
			GF_LOG(GF_LOG_INFO, GF_LOG_FILTER, ("Filter %s PID %s event %s but PID is not playing, discarding\n", f->name, evt->base.on_pid->name, gf_filter_event_name(evt->base.type)));
		} else {
			GF_LOG(GF_LOG_INFO, GF_LOG_FILTER, ("Filter %s PID %s event %s but PID has playing destinations, discarding\n", f->name, evt->base.on_pid->name, gf_filter_event_name(evt->base.type)));
		}

		gf_mx_p(f->tasks_mx);
		for (i=0; i<pid->num_destinations; i++) {
			GF_FilterPidInst *pidi = (GF_FilterPidInst *) gf_list_get(pid->destinations, i);
			//don't forget we pre-processed stop by incrementing the discard counter and setting discard_packets on pid instances
			//undo this
			if (pidi->discard_packets) {
				safe_int_dec(&pidi->discard_packets);
			}
		}
		if (!evt->base.on_pid->pid->is_playing) {
			if ((f->num_input_pids==f->num_output_pids) && (f->num_input_pids==1)) {
				gf_filter_pid_set_discard(gf_list_get(f->input_pids, 0), GF_TRUE);
			}
			if (pid->not_connected)
				pid->not_connected = 2;
		}
		gf_mx_v(f->tasks_mx);
		free_evt(evt);
		return;
	}
	//do not allow pause if already paused
	else if ((nb_paused>1) && (evt->base.type == GF_FEVT_PAUSE) ) {
		GF_LOG(GF_LOG_INFO, GF_LOG_FILTER, ("Filter %s PID %s event %s but PID is already paused, discarding\n", f->name, evt->base.on_pid->name, gf_filter_event_name(evt->base.type)));
		free_evt(evt);
		return;
	}
	//do not allow resume if some instances are still paused
	else if (nb_paused && (evt->base.type == GF_FEVT_RESUME) ) {
		GF_LOG(GF_LOG_INFO, GF_LOG_FILTER, ("Filter %s PID %s event %s but some PID instances are still paused, discarding\n", f->name, evt->base.on_pid->name, gf_filter_event_name(evt->base.type)));
		free_evt(evt);
		return;
	}
	//cancel connect failure if some destinations are successfully connected
	else if ((evt->base.type==GF_FEVT_CONNECT_FAIL) && evt->base.on_pid->is_playing) {
		free_evt(evt);
		return;
	}
	//otherwise process
	else {
		//reset EOS to false on source switch before executing the event, so that a filter may set a pid to EOS in the callback
		if (evt->base.type==GF_FEVT_SOURCE_SWITCH) {
			//if session has been aborted, cancel event - this avoids the dashin requesting a source switch on a source that is not yet over
			if (f->session->in_final_flush) {
				free_evt(evt);
				return;
			}
			for (i=0; i<f->num_output_pids; i++) {
				GF_FilterPid *apid = gf_list_get(f->output_pids, i);
				apid->has_seen_eos = GF_FALSE;
				gf_filter_pid_check_unblock(apid);
			}
		}

		if (f->freg->process_event) {
			FSESS_CHECK_THREAD(f)
			canceled = f->freg->process_event(f, evt);
		}
		if (!canceled && (evt->base.type==GF_FEVT_STOP) && evt->play.forced_dash_segment_switch) {
			GF_FilterPidInst *pid_inst = gf_list_get(f->input_pids, 0);
			//input is source filter, cancel
			if (pid_inst && ((pid_inst->pid->filter->num_input_pids==0) || (pid_inst->pid->filter->freg->flags & GF_FS_REG_ACT_AS_SOURCE))) {
				canceled = GF_TRUE;
				forced_cancel = GF_TRUE;
			}
		}
	}

	GF_LOG(GF_LOG_INFO, GF_LOG_FILTER, ("Filter %s PID %s processed event %s - canceled %s\n", f->name, evt->base.on_pid ? evt->base.on_pid->name : "none", gf_filter_event_name(evt->base.type), canceled ? "yes" : "no" ));

	if (evt->base.on_pid && ((evt->base.type == GF_FEVT_STOP) || (evt->base.type==GF_FEVT_SOURCE_SEEK) || (evt->base.type==GF_FEVT_PLAY)) ) {
		Bool do_reset = GF_TRUE;
		GF_FilterPidInst *p = (GF_FilterPidInst *) evt->base.on_pid;
		GF_FilterPid *pid = p->pid;
		gf_mx_p(pid->filter->tasks_mx);
		//we need to force a PID reset when the first PLAY is > 0, since some filters may have dispatched packets during the initialization
		//phase
		if (evt->base.type==GF_FEVT_PLAY) {
			pid->is_playing = GF_TRUE;
			pid->filter->nb_pids_playing++;
			if (pid->initial_play_done) {
				do_reset = GF_FALSE;
			} else {
				pid->initial_play_done = GF_TRUE;
				if (evt->play.start_range < 0.1)
					do_reset = GF_FALSE;
			}
		} else if (evt->base.type==GF_FEVT_STOP) {
			pid->is_playing = GF_FALSE;
			pid->filter->nb_pids_playing--;

			if (pid->not_connected)
				pid->not_connected = 2;
		} else if (evt->base.type==GF_FEVT_SOURCE_SEEK) {
			pid->is_playing = GF_TRUE;
			pid->filter->nb_pids_playing++;
		}
		for (i=0; i<pid->num_destinations && do_reset; i++) {
			GF_FilterPidInst *pidi = gf_list_get(pid->destinations, i);
			pidi->last_clock_type = 0;

			if (!pidi->discard_packets) {
				safe_int_inc(&pidi->discard_packets);
			}

			safe_int_inc(& pid->filter->stream_reset_pending );

			gf_mx_v(pid->filter->tasks_mx);

			//post task on destination filter
			if (evt->base.type==GF_FEVT_STOP)
				gf_fs_post_task(pidi->filter->session, gf_filter_pid_reset_stop_task, pidi->filter, NULL, "reset_stop_pid", pidi);
			else
				gf_fs_post_task(pidi->filter->session, gf_filter_pid_reset_task, pidi->filter, NULL, "reset_pid", pidi);

			gf_mx_p(pid->filter->tasks_mx);
		}
		pid->nb_reaggregation_pending = 0;
		gf_mx_v(pid->filter->tasks_mx);
	}
	
	gf_mx_p(f->tasks_mx);

	//after  play or seek, request a process task for source filters or filters having pending packets
	if (!f->num_input_pids || f->pending_packets) {
		if ((evt->base.type==GF_FEVT_PLAY) || (evt->base.type==GF_FEVT_SOURCE_SEEK)) {
			gf_filter_post_process_task(f);
		}
	}

	//quick hack for filters with one input pid and one outout pid, set discard on/off on the input
	//this avoids cases like TS demux dispatching data to inactive filters not checking their input
	//which ends up in session deadlock (filter still flagged as active and with pending packets)
	//if more than one input or more than one output, only the filter can decide what to do if some of the
	//streams are active and other not
	if ((f->num_input_pids==f->num_output_pids) && (f->num_input_pids==1)) {
		GF_FilterPidInst *apidi = gf_list_get(f->input_pids, 0);
		if (apidi->pid) {
			//unlock before setting discard to avoid deadlocks during shutdown
			gf_mx_v(f->tasks_mx);
			if (evt->base.type==GF_FEVT_STOP) {
				if (forced_cancel) {
					//we stop propagating the event to the source, but we must reset the source pid
					gf_filter_pid_set_discard((GF_FilterPid *)apidi, GF_TRUE);
//					gf_filter_pid_set_discard((GF_FilterPid *)apidi, GF_FALSE);
				} else if (!canceled) {
					gf_filter_pid_set_discard((GF_FilterPid *)apidi, GF_TRUE);
				}
			} else if (evt->base.type==GF_FEVT_PLAY) {
				gf_filter_pid_set_discard((GF_FilterPid *)apidi, GF_FALSE);
			}
			gf_mx_p(f->tasks_mx);
		}
	}
	gf_mx_v(f->tasks_mx);

	if ((evt->base.type==GF_FEVT_PLAY) || (evt->base.type==GF_FEVT_SET_SPEED)) {
		if (evt->base.on_pid) {
			u32 scaler = (u32)  ( (evt->play.speed<0) ? -evt->play.speed : evt->play.speed ) * GF_FILTER_SPEED_SCALER;
			if (!scaler) scaler = GF_FILTER_SPEED_SCALER;
			if (scaler != evt->base.on_pid->playback_speed_scaler) {
				u32 prev_scaler = evt->base.on_pid->playback_speed_scaler;
				evt->base.on_pid->playback_speed_scaler = scaler;
				//lowering speed, we may need to trigger blocking
				if (scaler<prev_scaler)
					gf_filter_pid_would_block(evt->base.on_pid);
				//increasing speed, we may want to unblock
				else
					gf_filter_pid_check_unblock(evt->base.on_pid);
			}
		}
	}

	//no more input pids
	gf_mx_p(f->tasks_mx);
	count = f->num_input_pids;
	if (count==0) canceled = GF_TRUE;

	if (canceled) {
		free_evt(evt);
		gf_mx_v(f->tasks_mx);
		return;
	}

	if (!task->pid) dispatched_filters = gf_list_new();

	//otherwise forward event to each input PID
	for (i=0; i<count; i++) {
		GF_FilterEvent *an_evt;
		GF_FilterPidInst *pid_inst = gf_list_get(f->input_pids, i);
		GF_FilterPid *pid = pid_inst->pid;
		if (!pid) continue;

		if (dispatched_filters) {
			if (gf_list_find(dispatched_filters, pid_inst->pid->filter) >=0 )
				continue;

			gf_list_add(dispatched_filters, pid_inst->pid->filter);
		}

		//mark pid instance as about to be reset to avoid processing PID destroy task before
		if ((evt->base.type == GF_FEVT_STOP) || (evt->base.type==GF_FEVT_SOURCE_SEEK)) {
			safe_int_inc(&pid_inst->discard_packets);
		}

		an_evt = dup_evt(evt);
		an_evt->base.on_pid = task->pid ? pid : NULL;

		safe_int_inc(&pid->filter->num_events_queued);
		
		gf_fs_post_task_class(pid->filter->session, gf_filter_pid_send_event_downstream, pid->filter, task->pid ? (GF_FilterPid *) pid_inst : NULL, "downstream_event", an_evt, TASK_TYPE_EVENT);
	}
	gf_mx_v(f->tasks_mx);
	if (dispatched_filters) gf_list_del(dispatched_filters);
	free_evt(evt);
	return;
}
