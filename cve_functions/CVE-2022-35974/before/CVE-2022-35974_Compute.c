  void Compute(OpKernelContext* ctx) override {
    const Tensor& input = ctx->input(0);
    const float input_min_float = ctx->input(1).flat<float>()(0);
    const float input_max_float = ctx->input(2).flat<float>()(0);
    Tensor* output = nullptr;
    OP_REQUIRES_OK(ctx, ctx->allocate_output(0, input.shape(), &output));
    Tensor* output_min = nullptr;
    OP_REQUIRES_OK(ctx, ctx->allocate_output(1, TensorShape({}), &output_min));
    Tensor* output_max = nullptr;
    OP_REQUIRES_OK(ctx, ctx->allocate_output(2, TensorShape({}), &output_max));

    // See QuantizationRangeOp as well, which has a copy of this logic.
    auto input_array = input.flat<T1>();
    const int32_t input_lowest_quantized =
        static_cast<int32>(Eigen::NumTraits<T1>::lowest());
    const int32_t input_highest_quantized =
        static_cast<int32>(Eigen::NumTraits<T1>::highest());
    T1 actual_min_quantized = input_highest_quantized;
    T1 actual_max_quantized = input_lowest_quantized;
    for (int i = 0; i < input_array.size(); ++i) {
      const T1 value = input_array(i);
      actual_min_quantized = std::min(actual_min_quantized, value);
      actual_max_quantized = std::max(actual_max_quantized, value);
    }
    // We want to make sure that the minimum is no larger than zero, so that the
    // convolution operation can run efficiently.
    const float actual_min_float =
        std::min(0.0f, QuantizedToFloat(actual_min_quantized, input_min_float,
                                        input_max_float));
    const float actual_max_float = QuantizedToFloat(
        actual_max_quantized, input_min_float, input_max_float);

#if 0
    // This is the reference, non-eigen implementation:
    auto output_array = output->flat<T2>();
    RequantizeManyInNewRange<T1, T2>(input_array.data(), input_array.size(),
                                     input_min_float, input_max_float,
                                     actual_min_float, actual_max_float,
                                     output_array.data());
#endif

    if (input_array.size() > 0) {
      if (meta::IsSupportedAndEnabled() && std::is_same<T1, qint32>() &&
          std::is_same<T2, quint8>()) {
        auto input_i32_array = input.flat<qint32>();
        meta::Requantize(ctx, input_i32_array.data(), input_i32_array.size(),
                         input_min_float, input_max_float, actual_min_float,
                         actual_max_float, output->flat<quint8>().data());
      } else {
        RequantizeManyInNewRangeUsingEigen<T1, T2>(
            ctx->eigen_device<CPUDevice>(), input, input_min_float,
            input_max_float, actual_min_float, actual_max_float, output);
      }
    }

    output_min->flat<float>().setConstant(actual_min_float);
    output_max->flat<float>().setConstant(actual_max_float);
  }
