static void ggml_barrier(struct ggml_compute_state_shared * shared) {
    if (shared->n_threads == 1) {
        return;
    }

    atomic_int * n_barrier = &shared->n_barrier;
    atomic_int * n_barrier_passed = &shared->n_barrier_passed;

    int n_threads = shared->n_threads;
    int passed_old = atomic_load(n_barrier_passed);

    if (atomic_fetch_add(n_barrier, 1) == n_threads - 1) {
        // last thread
        atomic_store(n_barrier, 0);
        atomic_fetch_add(n_barrier_passed, 1);
    } else {
        // wait for other threads
        const int n_spin_before_sleep = 100000;
        while (true) {
            for (int i = 0; i < n_spin_before_sleep; i++) {
                if (atomic_load(n_barrier_passed) != passed_old) {
                    return;
                }
            #if defined(__SSE3__)
                _mm_pause();
            #endif
            }
            sched_yield();
        }
    }
}
