static int send_other_control_frames(quicly_conn_t *conn, quicly_send_context_t *s)
{
    int ret;

    /* respond to all pending received PATH_CHALLENGE frames */
    if (conn->egress.path_challenge.head != NULL) {
        do {
            struct st_quicly_pending_path_challenge_t *c = conn->egress.path_challenge.head;
            if ((ret = do_allocate_frame(conn, s, QUICLY_PATH_CHALLENGE_FRAME_CAPACITY, ALLOCATE_FRAME_TYPE_NON_ACK_ELICITING)) !=
                0)
                return ret;
            s->dst = quicly_encode_path_challenge_frame(s->dst, c->is_response, c->data);
            if (c->is_response) {
                ++conn->super.stats.num_frames_sent.path_response;
            } else {
                ++conn->super.stats.num_frames_sent.path_challenge;
            }
            conn->egress.path_challenge.head = c->next;
            free(c);
        } while (conn->egress.path_challenge.head != NULL);
        conn->egress.path_challenge.tail_ref = &conn->egress.path_challenge.head;
        s->target.full_size = 1; /* datagrams carrying PATH_CHALLENGE / PATH_RESPONSE have to be full-sized */
    }

    /* MAX_STREAMS */
    if ((ret = send_max_streams(conn, 1, s)) != 0)
        return ret;
    if ((ret = send_max_streams(conn, 0, s)) != 0)
        return ret;

    /* MAX_DATA */
    if (should_send_max_data(conn)) {
        quicly_sent_t *sent;
        if ((ret = allocate_ack_eliciting_frame(conn, s, QUICLY_MAX_DATA_FRAME_CAPACITY, &sent, on_ack_max_data)) != 0)
            return ret;
        uint64_t new_value = conn->ingress.max_data.bytes_consumed + conn->super.ctx->transport_params.max_data;
        s->dst = quicly_encode_max_data_frame(s->dst, new_value);
        quicly_maxsender_record(&conn->ingress.max_data.sender, new_value, &sent->data.max_data.args);
        ++conn->super.stats.num_frames_sent.max_data;
        QUICLY_PROBE(MAX_DATA_SEND, conn, conn->stash.now, new_value);
        QUICLY_LOG_CONN(max_data_send, conn, { PTLS_LOG_ELEMENT_UNSIGNED(maximum, new_value); });
    }

    /* DATA_BLOCKED */
    if (conn->egress.data_blocked == QUICLY_SENDER_STATE_SEND && (ret = send_data_blocked(conn, s)) != 0)
        return ret;

    /* STREAMS_BLOCKED */
    if ((ret = send_streams_blocked(conn, 1, s)) != 0)
        return ret;
    if ((ret = send_streams_blocked(conn, 0, s)) != 0)
        return ret;

    { /* NEW_CONNECTION_ID */
        size_t i, size = quicly_local_cid_get_size(&conn->super.local.cid_set);
        for (i = 0; i < size; i++) {
            /* PENDING CIDs are located at the front */
            struct st_quicly_local_cid_t *c = &conn->super.local.cid_set.cids[i];
            if (c->state != QUICLY_LOCAL_CID_STATE_PENDING)
                break;
            if ((ret = send_new_connection_id(conn, s, c)) != 0)
                break;
        }
        quicly_local_cid_on_sent(&conn->super.local.cid_set, i);
        if (ret != 0)
            return ret;
    }

    { /* RETIRE_CONNECTION_ID */
        size_t i, size = quicly_retire_cid_get_num_pending(&conn->egress.retire_cid);
        for (i = 0; i < size; i++) {
            uint64_t sequence = conn->egress.retire_cid.sequences[i];
            if ((ret = send_retire_connection_id(conn, s, sequence)) != 0)
                break;
        }
        quicly_retire_cid_shift(&conn->egress.retire_cid, i);
        if (ret != 0)
            return ret;
    }

    return 0;
}
