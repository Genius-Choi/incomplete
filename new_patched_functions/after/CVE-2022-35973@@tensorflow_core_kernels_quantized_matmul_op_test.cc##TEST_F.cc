TEST_F(QuantizedMatMulTest, Medium_WithParams) {
  const bool transpose_a = true;
  const bool transpose_b = false;
  TF_ASSERT_OK(NodeDefBuilder("quantized_mat_mul_op", "QuantizedMatMul")
                   .Input(FakeInput(DT_QUINT8))
                   .Input(FakeInput(DT_QUINT8))
                   .Input(FakeInput(DT_FLOAT))
                   .Input(FakeInput(DT_FLOAT))
                   .Input(FakeInput(DT_FLOAT))
                   .Input(FakeInput(DT_FLOAT))
                   .Attr("Toutput", DataTypeToEnum<qint32>::v())
                   .Attr("transpose_a", transpose_a)
                   .Attr("transpose_b", transpose_b)
                   .Finalize(node_def()));
  TF_ASSERT_OK(InitOp());

  const int a_rows = 8;
  const int a_cols = 8;
  const float a_min = -2164.25f;
  const float a_max = 2006.27f;
  Tensor a_float(DT_FLOAT, {a_rows, a_cols});
  test::FillValues<float>(
      &a_float,
      {-1014.12, -157.382, -810.17,  1435.28,  1016.37,  219.684,  -316.054,
       -2164.25, 2006.27,  -547.444, 857.376,  404.376,  9.72115,  332.588,
       194.385,  -286.57,  26.062,   23.1125,  110.436,  247.055,  -127.683,
       -376.275, -124.81,  -846.826, -77.1507, 305.581,  -202.747, 12.9528,
       9.64886,  872.686,  40.9069,  197.816,  44.16,    -306.768, -1457.52,
       -368.939, -1049.42, -486.353, 1745.87,  95.7695,  395.773,  -254.333,
       -404.27,  787.16,   -2.44114, 199.37,   -1024.08, 784.901,  235.055,
       -42.7295, 241.498,  -245.365, 470.763,  186.159,  186.579,  -220.163,
       1304.58,  386.272,  -358.853, -755.996, 360.109,  -866.007, 55.2828,
       -508.801});
  Tensor a_quantized = FloatTensorToQuantized<quint8>(a_float, a_min, a_max);

  const int b_rows = 8;
  const int b_cols = 8;
  const float b_min = -0.739539f;
  const float b_max = 0.641057f;
  Tensor b_float(DT_FLOAT, {b_rows, b_cols});
  test::FillValues<float>(
      &b_float,
      {-0.294619, -0.0670519, 0.261507,   -0.126274, 0.127229,   -0.176945,
       -0.251223, 0.231086,   0.453694,   0.415666,  -0.288733,  0.508717,
       0.211551,  0.0435907,  -0.582383,  -0.308779, 0.0696883,  -0.438122,
       0.114,     0.433964,   0.109883,   0.284931,  -0.149661,  0.108657,
       0.458333,  -0.130231,  -0.35805,   -0.123206, -0.437968,  0.0282411,
       0.628818,  -0.0522173, -0.0233403, 0.124863,  0.217165,   0.262294,
       -0.171005, -0.254693,  -0.200433,  -0.287354, 0.488166,   -0.0354688,
       -0.118091, -0.590444,  0.491537,   -0.739539, 0.083117,   0.282482,
       0.275269,  -0.36574,   0.107476,   0.0511428, -0.136887,  -0.0149852,
       -0.259694, 0.641057,   0.264054,   -0.295126, -0.0218791, 0.361211,
       0.012448,  0.0709718,  -0.392394,  -0.434215});
  Tensor b_quantized = FloatTensorToQuantized<quint8>(b_float, b_min, b_max);

  AddInputFromArray<quint8>(a_quantized.shape(), a_quantized.flat<quint8>());
  AddInputFromArray<quint8>(b_quantized.shape(), b_quantized.flat<quint8>());
  AddInputFromArray<float>(TensorShape({}), {a_min});
  AddInputFromArray<float>(TensorShape({}), {a_max});
  AddInputFromArray<float>(TensorShape({}), {b_min});
  AddInputFromArray<float>(TensorShape({}), {b_max});
  TF_ASSERT_OK(RunOpKernel());

  Tensor expected_float(DT_FLOAT, {a_cols, b_cols});
  test::FillValues<float>(
      &expected_float,
      {1776.82f,  421.058f,  -854.308f, 1430.65f,  503.105f,  57.2744f,
       -1514.97f, -1163.66f, -87.0979f, -394.577f, -39.4983f, -79.1938f,
       -329.029f, 313.475f,  446.929f,  -59.5855f, 350.837f,  238.655f,
       -609.21f,  350.499f,  192.238f,  847.576f,  -103.177f, 185.886f,
       -90.5335f, 200.787f,  99.1981f,  -717.076f, 763.815f,  -703.726f,
       -125.164f, 732.325f,  -51.5303f, -418.826f, 60.0783f,  -299.658f,
       231.41f,   72.0622f,  -289.244f, 663.776f,  391.177f,  294.415f,
       -484.148f, -677.932f, -180.342f, -194.764f, 761.715f,  553.061f,
       -283.355f, 321.109f,  351.269f,  1171.7f,   -857.497f, 343.804f,
       -494.599f, -844.119f, 725.237f,  586.052f,  -735.013f, -897.723f,
       -122.434f, -502.907f, 1264.6f,   -239.991f});

  const Tensor& output_quantized = *GetOutput(0);
  const float output_min = GetOutput(1)->flat<float>()(0);
  const float output_max = GetOutput(2)->flat<float>()(0);
  Tensor output_float =
      QuantizedTensorToFloat<qint32>(output_quantized, output_min, output_max);
  test::ExpectTensorNear<float>(expected_float, output_float, 15.0);
}
