void RemoteCallOp::ComputeAsync(OpKernelContext* ctx, DoneCallback done) {
  FunctionLibraryRuntime* lib = ctx->function_library();
  OP_REQUIRES_ASYNC(ctx, lib != nullptr,
                    errors::Internal("No function library is provided."), done);

  const string& source_device = lib->device()->name();
  const Tensor* target;
  OP_REQUIRES_OK_ASYNC(ctx, ctx->input("target", &target), done);

  FunctionTarget function_target;
  OP_REQUIRES_OK_ASYNC(
      ctx,
      DeviceNameUtils::CanonicalizeDeviceName(
          target->scalar<tstring>()(), source_device, &function_target.first),
      done);
  function_target.second = lib;

  const string& target_device = function_target.first;
  const string& func_name = func_.name();

  FunctionLibraryRuntime::Handle handle;
  {
    mutex_lock l(mu_);
    auto cached_entry = handle_cache_.find(function_target);
    if (cached_entry != handle_cache_.end()) {
      handle = cached_entry->second;
    } else {
      VLOG(1) << "Instantiating " << func_name << " on " << target_device;
      profiler::TraceMe activity(
          [&] {
            return strings::StrCat("RemoteCall: Instantiate: ", func_name,
                                   " on ", target_device);
          },
          profiler::TraceMeLevel::kInfo);
      FunctionLibraryRuntime::InstantiateOptions instantiate_opts;
      const auto* config = (ctx->function_library())
                               ? ctx->function_library()->config_proto()
                               : nullptr;
      if (config) {
        instantiate_opts.config_proto = *config;
      }
      instantiate_opts.target = target_device;
      OP_REQUIRES_OK_ASYNC(ctx,
                           lib->Instantiate(func_name, AttrSlice(&func_.attr()),
                                            instantiate_opts, &handle),
                           done);
      auto insert_result = handle_cache_.insert({function_target, handle});
      CHECK(insert_result.second) << "Insert unsuccessful.";
      VLOG(1) << "Instantiated " << func_name << " on " << target_device
              << ", resulting in handle: " << handle << " flr: " << lib;
    }
  }

  OpInputList arguments;
  OP_REQUIRES_OK_ASYNC(ctx, ctx->input_list("args", &arguments), done);

  FunctionLibraryRuntime::Options opts;
  opts.runner = nullptr;  // Use default runner at remote device.
  opts.run_all_kernels_inline = ctx->run_all_kernels_inline();
  opts.source_device = source_device;
  if (opts.source_device != target_device) {
    opts.remote_execution = true;
  }
  opts.create_rendezvous = true;
  CancellationManager* cancel_mgr = nullptr;
  if (ctx->cancellation_manager() != nullptr) {
    cancel_mgr = new CancellationManager(ctx->cancellation_manager());
  }
  opts.cancellation_manager = cancel_mgr;
  opts.collective_executor = ctx->collective_executor();
  std::vector<Tensor> args(arguments.begin(), arguments.end());
  opts.args_alloc_attrs.reserve(input_dtypes_.size());
  for (const auto& dtype : input_dtypes_) {
    AllocatorAttributes arg_alloc_attrs;
    arg_alloc_attrs.set_on_host(DataTypeAlwaysOnHost(dtype));
    opts.args_alloc_attrs.push_back(arg_alloc_attrs);
  }
  opts.rets_alloc_attrs.reserve(output_dtypes_.size());
  DCHECK(!return_type_.IsInitialized() ||
         (return_type_.type_id() == TFT_UNSET) ||
         (output_dtypes_.size() == return_type_.args_size()))
      << "RemoteCall op has a full type information for "
      << return_type_.args_size() << " outputs but the number of outputs is "
      << output_dtypes_.size();
  for (const auto& dtype : output_dtypes_) {
    AllocatorAttributes ret_alloc_attrs;
    bool on_host = DataTypeAlwaysOnHost(dtype);
    if (return_type_.IsInitialized() && (return_type_.type_id() != TFT_UNSET)) {
      DCHECK(return_type_.type_id() == TFT_PRODUCT)
          << return_type_.DebugString();
      FullTypeDef ftd = full_type::GetArgDefaultUnset(
          return_type_, opts.rets_alloc_attrs.size());
      if (full_type::IsHostMemoryType(ftd)) {
        on_host = true;
      }
      VLOG(5) << "FulltypeDef for RemoteCall output="
              << opts.rets_alloc_attrs.size()
              << ", IsHostMemoryType=" << full_type::IsHostMemoryType(ftd)
              << ":\n"
              << ftd.DebugString();
    }
    ret_alloc_attrs.set_on_host(on_host);
    opts.rets_alloc_attrs.push_back(ret_alloc_attrs);
  }
  auto* rets = new std::vector<Tensor>;
  VLOG(1) << "Running " << func_name << " on " << target_device
          << " with handle: " << handle;
  profiler::TraceMe trace_me(
      [&] {
        return profiler::TraceMeEncode(
            "RemoteCallOp",
            {{"func_name", func_name}, {"device", target_device}});
      },
      profiler::TraceMeLevel::kInfo);
  lib->Run(
      opts, handle, args, rets,
      [rets, done = std::move(done), func_name, ctx, cancel_mgr,
       target_device = std::move(function_target.first)](const Status& status) {
        profiler::TraceMe activity(
            [&] {
              return profiler::TraceMeEncode(
                  "RemoteCallOpDone",
                  {{"func_name", func_name}, {"device", target_device}});
            },
            profiler::TraceMeLevel::kInfo);
        if (!status.ok()) {
          ctx->SetStatus(status);
        } else {
          for (size_t i = 0; i < rets->size(); ++i) {
            ctx->set_output(i, std::move((*rets)[i]));
          }
        }
        delete cancel_mgr;
        delete rets;
        done();
      });
}
