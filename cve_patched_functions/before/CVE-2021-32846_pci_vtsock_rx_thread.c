static void *pci_vtsock_rx_thread(void *vsc)
{
	struct pci_vtsock_softc *sc = vsc;
	struct vqueue_info *vq = &sc->vssc_vqs[VTSOCK_QUEUE_RX];
	fd_set rfd;
	bool poll_socks = true;
	struct pci_vtsock_sock *s, *ts;
	LIST_HEAD(rx_queue, pci_vtsock_sock) queue;
	LIST_HEAD(rx_closing_queue, pci_vtsock_sock) closing_queue;

	assert(sc);
	assert(sc->rx_wake_fd != -1);

	pthread_setname_np("vsock:rx");

rx_done:

	while (1) {
		int nrfd, maxfd, nr;
		bool did_some_work = true;
		bool pending_credit_updates = false;
		struct timeval zero_timeout = {
			.tv_sec = 0,
			.tv_usec = 0,
		};

		FD_ZERO(&rfd);

		LIST_INIT(&queue);
		LIST_INIT(&closing_queue);

		FD_SET(sc->rx_wake_fd, &rfd);
		maxfd = sc->rx_wake_fd;
		nrfd = 1;

		pthread_rwlock_rdlock(&sc->list_rwlock);
		LIST_FOREACH(s, &sc->inuse_list, list) {
			bool polling = true;
			uint32_t peer_free;

			get_sock(s);

			if (s->state == SOCK_CLOSING_RX) { /* Closing comes through here */
				assert(s->local_shutdown == VIRTIO_VSOCK_FLAG_SHUTDOWN_ALL ||
				       s->peer_shutdown == VIRTIO_VSOCK_FLAG_SHUTDOWN_ALL);

				DPRINTF(("RX: Closing sock %p fd %d local %"PRIx32" peer %"PRIx32"\n",
					 (void *)s, s->fd,
					 s->local_shutdown,
					 s->peer_shutdown));
				PPRINTF(("RX: SOCK closed (%d) "PRIaddr" <=> "PRIaddr"\n",
					 s->fd,
					 FMTADDR(s->local_addr), FMTADDR(s->peer_addr)));

				close(s->fd);
				s->fd = -1;

				/* Cannot move to free list with only
				 * rdlock on list_rwlock, queue on a
				 * local list */
				LIST_INSERT_HEAD(&closing_queue, s, rx_queue);

				put_sock(s);
				continue;
			}

			if (s->state != SOCK_CONNECTED || !poll_socks) {
				put_sock(s);
				continue;
			}

			if (s->local_shutdown & VIRTIO_VSOCK_FLAG_SHUTDOWN_TX)
				polling = false;

			if (s->peer_shutdown & VIRTIO_VSOCK_FLAG_SHUTDOWN_RX)
				polling = false;

			if (s->credit_update_required)
				pending_credit_updates = true;

			assert(s->fd >= 0);
			peer_free = s->peer_buf_alloc - (s->rx_cnt - s->peer_fwd_cnt);
			DPRINTF(("RX: sock %p (%d): peer free = %"PRId32"\n",
				 (void*)s, s->fd, peer_free));
			if (peer_free == 0)
				polling = false;

			if (polling) {
				FD_SET(s->fd, &rfd);
				maxfd = max_fd(s->fd, maxfd);
				nrfd++;
			}

			if (polling || s->credit_update_required)
				LIST_INSERT_HEAD(&queue, s, rx_queue);

			put_sock(s);
		}
		pthread_rwlock_unlock(&sc->list_rwlock);

		if (!LIST_EMPTY(&closing_queue)) {
			pthread_rwlock_wrlock(&sc->list_rwlock);

			LIST_FOREACH(s, &closing_queue, rx_queue) {
				get_sock(s);
				free_sock(sc, s);
			}

			pthread_rwlock_unlock(&sc->list_rwlock);
		}

		/* Unlocked during select */
		assert(maxfd < FD_SETSIZE);

		DPRINTF(("RX: *** thread selecting on %d fds (socks: %s)\n",
			 nrfd, poll_socks ? "yes" : "no"));

		/*
		 * If we have pending_credit_updates then pass zero
		 * timeout to poll the fds but don't block so we will
		 * immediately handle whatever work we can, including
		 * the pending credit updates.
		 */
		nr = xselect("RX", maxfd + 1, &rfd, NULL, NULL,
			    pending_credit_updates ? &zero_timeout : NULL);
		if (nr < 0) continue;
		DPRINTF(("RX:\nRX: *** %d/%d fds are readable (descs: %s)\n",
			 nr, nrfd, vq_has_descs(vq) ? "yes" : "no"));

		if (FD_ISSET(sc->rx_wake_fd, &rfd)) {
			/* Eat the notification(s) */
			char dummy[128];
			ssize_t rd_dummy = read(sc->rx_wake_fd, &dummy, 128);
			assert(rd_dummy >= 1);
			/* Restart select now that we have some
			 * descriptors. It's possible that synchronous
			 * responses sent from the tx thread have
			 * eaten them all though, so check.
			 */
			DPRINTF(("RX: thread got %zd kicks (have descs: %s)\n",
				 rd_dummy, vq_has_descs(vq) ? "yes" : "no"));

// XXX need to check sockets in order to process the reply ring, so
// cannot make this tempting looking optimisation.
//
//			if (nr == 1) {
//				 /* Must have been the kicker fd, in
//				  * which case there is no point
//				  * checking the socks.
//				  */
//				DPRINTF(("RX: Kicked w/ no other fds -- restarting select()\n"));
//				goto rx_done;
//			}

			/* We might have some descriptors, so it might be worth polling the socks again */
			poll_socks = true;
		}

		if (!vq_has_descs(vq)) {
			DPRINTF(("RX: No descs -- restarting select()\n"));
			poll_socks = false; /* Don't poll socks next time */
			goto rx_done;
		}

		while (did_some_work) {
			int nr_data_rx = 0;
			bool more_replies_pending = true; /* Assume there is */
			did_some_work = false;

			DPRINTF(("RX: Handling pending replies first\n"));
			pthread_mutex_lock(&sc->reply_mtx);
			while (vq_has_descs(vq)) {
				more_replies_pending = rx_do_one_reply(sc, vq);
				if (!more_replies_pending) break;
				did_some_work = true;
			}
			pthread_mutex_unlock(&sc->reply_mtx);

			if (more_replies_pending) {
				DPRINTF(("RX: No more descriptors for pending replies\n"));
				poll_socks = false; /* Still replies to send, so don't handle socks yet */
				vq_endchains(vq, 1);
				goto rx_done;
			}

			DPRINTF(("RX: Checking all socks\n"));

			LIST_FOREACH_SAFE(s, &queue, rx_queue, ts) {
				/*
				 * Check for new replies in the reply
				 * ring frequently in order to avoid
				 * possible deadlock due to filling
				 * both vrings with data leaving no
				 * space for replies. See "Virtqueue
				 * Flow Control" in the spec.
				 */
				if (nr_data_rx++ >= 8) {
					bool replies_pending;
					pthread_mutex_lock(&sc->reply_mtx);
					replies_pending = !REPLY_RING_EMPTY(sc);
					pthread_mutex_unlock(&sc->reply_mtx);
					if (replies_pending) break;
				}

				get_sock(s);

				if (s->state != SOCK_CONNECTED) {
					LIST_REMOVE(s, rx_queue);
					put_sock(s);
					continue;
				}

				assert(s->fd >= 0);

				if (FD_ISSET(s->fd, &rfd)) {
					ssize_t bytes;
					DPRINTF(("RX: event on sock %p fd %d\n",
						 (void *)s, s->fd));
					bytes = pci_vtsock_proc_rx(sc, vq, s);
					if (bytes == -1) {
						/* Consumed all descriptors, stop */
						DPRINTF(("RX: No more descriptors\n"));
						vq_endchains(vq, 1);
						put_sock(s);
						goto rx_done;
					} else if (bytes == 0) {
						LIST_REMOVE(s, rx_queue);
						FD_CLR(s->fd, &rfd);
					} else {
						did_some_work = true;
					}
					/*
					 * If proc_rx returned >= 0
					 * then it is guaranteed to
					 * have sent something and
					 * thus a credit update is no
					 * longer required. We have
					 * handled the < 0 case above.
					 */
					assert(s->credit_update_required == false);
				} else if (s->credit_update_required) {
					if (send_credit_update(vq, s)) {
						s->credit_update_required = false;
					} else {
						/* Consumed all descriptors, stop */
						DPRINTF(("RX: No more descriptors\n"));
						vq_endchains(vq, 1);
						put_sock(s);
						goto rx_done;
					}
				} else {
					/* Was nothing to do */
					LIST_REMOVE(s, rx_queue);
				}

				put_sock(s);
			}
		}

		DPRINTF(("RX: All work complete\n"));
		vq_endchains(vq, 0);
	}
}
