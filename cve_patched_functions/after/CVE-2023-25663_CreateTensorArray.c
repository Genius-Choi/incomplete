  Status CreateTensorArray(OpKernelContext* ctx, ResourceMgr* rm,
                           Tensor* tensor_array_output_handle,
                           TensorArray** output_tensor_array) override {
    string container;
    string tensor_array_name;
    if (ctx->input_dtype(0) != DT_RESOURCE) {
      TF_RETURN_IF_ERROR(GetHandle(ctx, &container, &tensor_array_name));
      if (container != "_tensor_arrays") {
        return errors::InvalidArgument(
            "Input container should be '_tensor_arrays',  but received '",
            container, "'");
      }
    } else {
      container = "_tensor_arrays";
      const auto& resource = ctx->input(0).flat<ResourceHandle>()(0);
      if (StringPiece(resource.name()).substr(0, container.size()) !=
          container) {
        return errors::InvalidArgument("Wrong input container. ",
                                       resource.name());
      }
      tensor_array_name =
          string(StringPiece(resource.name()).substr(container.size()));
    }

    auto output_handle = tensor_array_output_handle->flat<tstring>();
    output_handle(0) = "_tensor_array_grads";
    output_handle(1) = strings::StrCat(tensor_array_name, "@", source_);

    TensorArray* tensor_array;
    TF_RETURN_IF_ERROR(ctx->step_container()->Lookup(
        rm, strings::StrCat(container, tensor_array_name), &tensor_array));
    core::ScopedUnref unref(tensor_array);

    // Once gradients are being calculated, the forward TensorArray
    // may no longer be resized by new Writes.
    tensor_array->DisableDynamicSize();

    int32_t array_size = 0;
    int32_t marked_size = 0;
    TF_RETURN_IF_ERROR(tensor_array->Size(&array_size));
    TF_RETURN_IF_ERROR(tensor_array->MarkedSize(&marked_size));

    if (array_size < 0) {
      return errors::InvalidArgument("ArraySize should be >= 0.");
    }
    if (!tensor_array->GradientsAllowed()) {
      return errors::InvalidArgument(
          "Unable to create a gradients TensorArray for ", tensor_array_name,
          ".  Perhaps you used the multiple_writes_aggregate flag on a "
          "previous write?  Gradient calculation is impossible when multiple "
          "writes are performed to the same index.");
    }
    TensorShape shape_to_prepend;
    auto element_shape = PartialTensorShape();
    if (ctx->num_inputs() > 2) {
      TF_RETURN_IF_ERROR(tensor::MakeShape(ctx->input(2), &shape_to_prepend));
      auto ta_element_shape = tensor_array->ElemShape();
      if (!ta_element_shape.unknown_rank()) {
        std::vector<int64_t> dims;
        for (auto dim : shape_to_prepend) {
          dims.push_back(dim.size);
        }
        for (auto dim : ta_element_shape) {
          dims.push_back(dim.size);
        }
        TF_RETURN_IF_ERROR(TensorShapeUtils::MakeShape(
            gtl::ArraySlice<int64_t>(dims), &element_shape));
      }
    } else {
      element_shape = tensor_array->ElemShape();
    }

    const auto key = strings::StrCat(output_handle(0), output_handle(1));
    auto creator = [key, tensor_array, array_size, marked_size, element_shape,
                    shape_to_prepend,
                    tensor_array_output_handle](TensorArray** ret) -> Status {
      *ret = new TensorArray(
          key, tensor_array->ElemType(), *tensor_array_output_handle,
          array_size, element_shape, tensor_array->HasIdenticalElementShapes(),
          false /* dynamic_size */, true /* multiple_writes_aggregate */,
          true /* is_grad */, marked_size /* marked_size */,
          true /* close_after_read */);
      return (*ret)->CopyShapesFrom(tensor_array, &shape_to_prepend);
    };

    Status s = ctx->step_container()->LookupOrCreate<TensorArray>(
        rm, key, output_tensor_array, creator);
    (*output_tensor_array)->Unref();

    return s;
  }
