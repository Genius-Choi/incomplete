static int lan78xx_bh(struct lan78xx_net *dev, int budget)
{
	struct sk_buff_head done;
	struct sk_buff *rx_buf;
	struct skb_data *entry;
	unsigned long flags;
	int work_done = 0;

	/* Pass frames received in the last NAPI cycle before
	 * working on newly completed URBs.
	 */
	while (!skb_queue_empty(&dev->rxq_overflow)) {
		lan78xx_skb_return(dev, skb_dequeue(&dev->rxq_overflow));
		++work_done;
	}

	/* Take a snapshot of the done queue and move items to a
	 * temporary queue. Rx URB completions will continue to add
	 * to the done queue.
	 */
	__skb_queue_head_init(&done);

	spin_lock_irqsave(&dev->rxq_done.lock, flags);
	skb_queue_splice_init(&dev->rxq_done, &done);
	spin_unlock_irqrestore(&dev->rxq_done.lock, flags);

	/* Extract receive frames from completed URBs and
	 * pass them to the stack. Re-submit each completed URB.
	 */
	while ((work_done < budget) &&
	       (rx_buf = __skb_dequeue(&done))) {
		entry = (struct skb_data *)(rx_buf->cb);
		switch (entry->state) {
		case rx_done:
			rx_process(dev, rx_buf, budget, &work_done);
			break;
		case rx_cleanup:
			break;
		default:
			netdev_dbg(dev->net, "rx buf state %d\n",
				   entry->state);
			break;
		}

		lan78xx_rx_urb_resubmit(dev, rx_buf);
	}

	/* If budget was consumed before processing all the URBs put them
	 * back on the front of the done queue. They will be first to be
	 * processed in the next NAPI cycle.
	 */
	spin_lock_irqsave(&dev->rxq_done.lock, flags);
	skb_queue_splice(&done, &dev->rxq_done);
	spin_unlock_irqrestore(&dev->rxq_done.lock, flags);

	if (netif_device_present(dev->net) && netif_running(dev->net)) {
		/* reset update timer delta */
		if (timer_pending(&dev->stat_monitor) && (dev->delta != 1)) {
			dev->delta = 1;
			mod_timer(&dev->stat_monitor,
				  jiffies + STAT_UPDATE_TIMER);
		}

		/* Submit all free Rx URBs */

		if (!test_bit(EVENT_RX_HALT, &dev->flags))
			lan78xx_rx_urb_submit_all(dev);

		/* Submit new Tx URBs */

		lan78xx_tx_bh(dev);
	}

	return work_done;
}
