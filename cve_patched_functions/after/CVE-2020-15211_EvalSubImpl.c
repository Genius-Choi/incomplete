void EvalSubImpl(TfLiteContext* context, TfLiteNode* node,
                 TfLiteSubParams* params, const OpData* data,
                 const TfLiteTensor* input1, const TfLiteTensor* input2,
                 bool requires_broadcast, TfLiteTensor* output) {
  data_type output_activation_min, output_activation_max;
  CalculateActivationRange(params->activation, &output_activation_min,
                           &output_activation_max);
  tflite::ArithmeticParams op_params;
  SetActivationParams(output_activation_min, output_activation_max, &op_params);

  switch (kernel_type) {
    case kReference:
      if (requires_broadcast) {
        reference_ops::BroadcastSubSlow(
            op_params, GetTensorShape(input1), GetTensorData<data_type>(input1),
            GetTensorShape(input2), GetTensorData<data_type>(input2),
            GetTensorShape(output), GetTensorData<data_type>(output));
      } else {
        reference_ops::SubWithActivation(
            op_params, GetTensorShape(input1), GetTensorData<data_type>(input1),
            GetTensorShape(input2), GetTensorData<data_type>(input2),
            GetTensorShape(output), GetTensorData<data_type>(output));
      }
      break;
    case kGenericOptimized:
    case kNeonOptimized:
      if (requires_broadcast) {
        optimized_ops::BroadcastSubSlow(
            op_params, GetTensorShape(input1), GetTensorData<data_type>(input1),
            GetTensorShape(input2), GetTensorData<data_type>(input2),
            GetTensorShape(output), GetTensorData<data_type>(output));
      } else {
        optimized_ops::SubWithActivation(
            op_params, GetTensorShape(input1), GetTensorData<data_type>(input1),
            GetTensorShape(input2), GetTensorData<data_type>(input2),
            GetTensorShape(output), GetTensorData<data_type>(output));
      }
      break;
  }
}
