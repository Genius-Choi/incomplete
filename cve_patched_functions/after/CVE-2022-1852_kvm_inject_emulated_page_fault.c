bool kvm_inject_emulated_page_fault(struct kvm_vcpu *vcpu,
				    struct x86_exception *fault)
{
	struct kvm_mmu *fault_mmu;
	WARN_ON_ONCE(fault->vector != PF_VECTOR);

	fault_mmu = fault->nested_page_fault ? vcpu->arch.mmu :
					       vcpu->arch.walk_mmu;

	/*
	 * Invalidate the TLB entry for the faulting address, if it exists,
	 * else the access will fault indefinitely (and to emulate hardware).
	 */
	if ((fault->error_code & PFERR_PRESENT_MASK) &&
	    !(fault->error_code & PFERR_RSVD_MASK))
		kvm_mmu_invalidate_gva(vcpu, fault_mmu, fault->address,
				       fault_mmu->root.hpa);

	/*
	 * A workaround for KVM's bad exception handling.  If KVM injected an
	 * exception into L2, and L2 encountered a #PF while vectoring the
	 * injected exception, manually check to see if L1 wants to intercept
	 * #PF, otherwise queuing the #PF will lead to #DF or a lost exception.
	 * In all other cases, defer the check to nested_ops->check_events(),
	 * which will correctly handle priority (this does not).  Note, other
	 * exceptions, e.g. #GP, are theoretically affected, #PF is simply the
	 * most problematic, e.g. when L0 and L1 are both intercepting #PF for
	 * shadow paging.
	 *
	 * TODO: Rewrite exception handling to track injected and pending
	 *       (VM-Exit) exceptions separately.
	 */
	if (unlikely(vcpu->arch.exception.injected && is_guest_mode(vcpu)) &&
	    kvm_x86_ops.nested_ops->handle_page_fault_workaround(vcpu, fault))
		return true;

	fault_mmu->inject_page_fault(vcpu, fault);
	return false;
}
